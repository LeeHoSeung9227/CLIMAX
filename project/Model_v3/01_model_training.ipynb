{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFeatureEngineering:\n",
    "    \"\"\"고도화된 특성 공학\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        self.selected_features = None\n",
    "        \n",
    "    def create_extreme_weather_features(self, df):\n",
    "        \"\"\"극한 기상 상태 이진 변수 생성\"\"\"\n",
    "        print(\"🌪️ 극한 기상 상태 변수 생성 중...\")\n",
    "        \n",
    "        # 온도 기반 극한 상태\n",
    "        if 'ta' in df.columns:\n",
    "            temp_q05 = df['ta'].quantile(0.05)  # 한파\n",
    "            temp_q95 = df['ta'].quantile(0.95)  # 폭염\n",
    "            \n",
    "            df['extreme_cold'] = (df['ta'] <= temp_q05).astype(int)\n",
    "            df['extreme_heat'] = (df['ta'] >= temp_q95).astype(int)\n",
    "            df['moderate_temp'] = ((df['ta'] > temp_q05) & (df['ta'] < temp_q95)).astype(int)\n",
    "            \n",
    "            print(f\"  한파 기준: ≤{temp_q05:.1f}°C ({df['extreme_cold'].sum():,}개)\")\n",
    "            print(f\"  폭염 기준: ≥{temp_q95:.1f}°C ({df['extreme_heat'].sum():,}개)\")\n",
    "        \n",
    "        # 강수 기반 극한 상태\n",
    "        if 'rn_hr1' in df.columns:\n",
    "            df['no_rain'] = (df['rn_hr1'] == 0).astype(int)\n",
    "            df['light_rain'] = ((df['rn_hr1'] > 0) & (df['rn_hr1'] <= 2)).astype(int)\n",
    "            df['heavy_rain'] = (df['rn_hr1'] > 10).astype(int)\n",
    "            df['extreme_rain'] = (df['rn_hr1'] > 30).astype(int)\n",
    "            \n",
    "            print(f\"  폭우 기준: >10mm ({df['heavy_rain'].sum():,}개)\")\n",
    "            print(f\"  극한 강수: >30mm ({df['extreme_rain'].sum():,}개)\")\n",
    "        \n",
    "        # 풍속 기반 극한 상태\n",
    "        if 'ws' in df.columns:\n",
    "            wind_q90 = df['ws'].quantile(0.90)\n",
    "            wind_q95 = df['ws'].quantile(0.95)\n",
    "            \n",
    "            df['calm_wind'] = (df['ws'] <= 1.0).astype(int)\n",
    "            df['strong_wind'] = (df['ws'] >= wind_q90).astype(int)\n",
    "            df['extreme_wind'] = (df['ws'] >= wind_q95).astype(int)\n",
    "            \n",
    "            print(f\"  강풍 기준: ≥{wind_q90:.1f}m/s ({df['strong_wind'].sum():,}개)\")\n",
    "            print(f\"  극한 풍속: ≥{wind_q95:.1f}m/s ({df['extreme_wind'].sum():,}개)\")\n",
    "        \n",
    "        # 습도 기반 극한 상태\n",
    "        if 'hm' in df.columns:\n",
    "            humidity_q05 = df['hm'].quantile(0.05)\n",
    "            humidity_q95 = df['hm'].quantile(0.95)\n",
    "            \n",
    "            df['extreme_dry'] = (df['hm'] <= humidity_q05).astype(int)\n",
    "            df['extreme_humid'] = (df['hm'] >= humidity_q95).astype(int)\n",
    "            \n",
    "            print(f\"  극건조: ≤{humidity_q05:.1f}% ({df['extreme_dry'].sum():,}개)\")\n",
    "            print(f\"  극습함: ≥{humidity_q95:.1f}% ({df['extreme_humid'].sum():,}개)\")\n",
    "        \n",
    "        # 복합 극한 상태\n",
    "        df['extreme_weather_any'] = (\n",
    "            df.get('extreme_cold', 0) | df.get('extreme_heat', 0) |\n",
    "            df.get('heavy_rain', 0) | df.get('extreme_wind', 0) |\n",
    "            df.get('extreme_dry', 0) | df.get('extreme_humid', 0)\n",
    "        ).astype(int)\n",
    "        \n",
    "        print(f\"  전체 극한 기상: {df['extreme_weather_any'].sum():,}개 ({df['extreme_weather_any'].mean()*100:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_detailed_time_features(self, df):\n",
    "        \"\"\"세분화된 시간대/계절 범주형 변수\"\"\"\n",
    "        print(\"⏰ 세분화된 시간 특성 생성 중...\")\n",
    "        \n",
    "        # 시간 관련 기본 특성\n",
    "        df['datetime'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        \n",
    "        # 세분화된 출퇴근 시간대\n",
    "        def get_detailed_time_period(hour):\n",
    "            if hour in [6, 7]:\n",
    "                return 'early_morning_rush'\n",
    "            elif hour in [8, 9]:\n",
    "                return 'morning_rush_peak'\n",
    "            elif hour == 10:\n",
    "                return 'morning_rush_end'\n",
    "            elif hour in [11, 12, 13, 14]:\n",
    "                return 'daytime'\n",
    "            elif hour in [15, 16]:\n",
    "                return 'afternoon_start'\n",
    "            elif hour in [17, 18]:\n",
    "                return 'evening_rush_start'\n",
    "            elif hour in [19, 20]:\n",
    "                return 'evening_rush_peak'\n",
    "            elif hour == 21:\n",
    "                return 'evening_rush_end'\n",
    "            elif hour in [22, 23]:\n",
    "                return 'night'\n",
    "            else:  # 0-5시\n",
    "                return 'late_night'\n",
    "        \n",
    "        df['time_period'] = df['hour'].apply(get_detailed_time_period)\n",
    "        \n",
    "        # 세분화된 계절\n",
    "        def get_detailed_season(month):\n",
    "            if month in [12, 1, 2]:\n",
    "                return 'winter'\n",
    "            elif month in [3, 4]:\n",
    "                return 'spring_early'\n",
    "            elif month == 5:\n",
    "                return 'spring_late'\n",
    "            elif month in [6, 7]:\n",
    "                return 'summer_early'\n",
    "            elif month == 8:\n",
    "                return 'summer_peak'\n",
    "            elif month in [9, 10]:\n",
    "                return 'autumn_early'\n",
    "            else:  # 11월\n",
    "                return 'autumn_late'\n",
    "        \n",
    "        df['detailed_season'] = df['month'].apply(get_detailed_season)\n",
    "        \n",
    "        # 주말/평일 세분화\n",
    "        df['day_type'] = df['dayofweek'].apply(\n",
    "            lambda x: 'weekend' if x >= 5 else 'weekday'\n",
    "        )\n",
    "        \n",
    "        # 월요일/금요일 효과\n",
    "        df['is_monday'] = (df['dayofweek'] == 0).astype(int)\n",
    "        df['is_friday'] = (df['dayofweek'] == 4).astype(int)\n",
    "        \n",
    "        # 순환적 시간 특성 (기존 + 추가)\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "        df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "        \n",
    "        print(f\"  세분화된 시간대: {df['time_period'].nunique()}개 카테고리\")\n",
    "        print(f\"  세분화된 계절: {df['detailed_season'].nunique()}개 카테고리\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_interaction_features(self, df):\n",
    "        \"\"\"상호작용 항 생성\"\"\"\n",
    "        print(\"🔗 상호작용 특성 생성 중...\")\n",
    "        \n",
    "        # 기상 변수 간 상호작용\n",
    "        if 'ta' in df.columns and 'hm' in df.columns:\n",
    "            # 체감온도 (온도 × 습도)\n",
    "            df['apparent_temp'] = df['ta'] * (1 + df['hm'] / 100)\n",
    "            df['temp_humidity_interaction'] = df['ta'] * df['hm']\n",
    "            print(\"  온도 × 습도 상호작용 생성\")\n",
    "        \n",
    "        if 'ta' in df.columns and 'ws' in df.columns:\n",
    "            # 풍속에 의한 체감온도\n",
    "            df['wind_chill'] = df['ta'] - df['ws'] * 2\n",
    "            df['temp_wind_interaction'] = df['ta'] * df['ws']\n",
    "            print(\"  온도 × 풍속 상호작용 생성\")\n",
    "        \n",
    "        if 'rn_hr1' in df.columns and 'ws' in df.columns:\n",
    "            # 비바람 효과\n",
    "            df['rain_wind_interaction'] = df['rn_hr1'] * df['ws']\n",
    "            print(\"  강수 × 풍속 상호작용 생성\")\n",
    "        \n",
    "        if 'rn_hr1' in df.columns and 'hm' in df.columns:\n",
    "            # 습도-강수 상호작용\n",
    "            df['rain_humidity_interaction'] = df['rn_hr1'] * df['hm']\n",
    "            print(\"  강수 × 습도 상호작용 생성\")\n",
    "        \n",
    "        # 시간-기상 상호작용\n",
    "        if 'ta' in df.columns:\n",
    "            df['temp_hour_interaction'] = df['ta'] * df['hour']\n",
    "            df['temp_season_interaction'] = df['ta'] * df['month']\n",
    "            print(\"  온도 × 시간 상호작용 생성\")\n",
    "        \n",
    "        # 극한 기상과 시간 상호작용\n",
    "        if 'extreme_weather_any' in df.columns:\n",
    "            df['extreme_weather_rush'] = df['extreme_weather_any'] * (\n",
    "                df['time_period'].isin(['morning_rush_peak', 'evening_rush_peak']).astype(int)\n",
    "            )\n",
    "            print(\"  극한기상 × 출퇴근시간 상호작용 생성\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_lag_features(self, df, target_col='congestion'):\n",
    "        \"\"\"시차 특성 생성\"\"\"\n",
    "        print(\"📈 기상 변수 시차 특성 생성 중 (혼잡도 시차 제외)...\")\n",
    "        \n",
    "        # 시간 정렬\n",
    "        df = df.sort_values(['station_name', 'tm']).reset_index(drop=True)\n",
    "        \n",
    "        # 기상 변수 시차 특성만 생성 (1일 전)\n",
    "        weather_vars = ['ta', 'ws', 'rn_hr1', 'hm']\n",
    "        for var in weather_vars:\n",
    "            if var in df.columns:\n",
    "                df[f'{var}_lag_24'] = df[var].shift(24)\n",
    "        \n",
    "        # 기상 변수 이동평균 특성 (3,6,12시간)\n",
    "        for var in weather_vars:\n",
    "            if var in df.columns:\n",
    "                for window in [3, 6, 12]:\n",
    "                    df[f'{var}_ma_{window}'] = df[var].rolling(\n",
    "                        window=window, min_periods=1\n",
    "                    ).mean()\n",
    "        \n",
    "        print(f\"  기상 시차 특성: {len(weather_vars)}개\")\n",
    "        print(f\"  기상 이동평균 특성: {len(weather_vars) * 3}개\")\n",
    "        print(\"  ✅ 혼잡도 과거 데이터는 제외됨\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSubwayModel:\n",
    "    \"\"\"고도화된 지하철 혼잡도 예측 모델\"\"\"\n",
    "    \n",
    "    def __init__(self, use_optuna=True, n_trials=100):\n",
    "        self.feature_engineer = EnhancedFeatureEngineering()\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.feature_importance = None\n",
    "        self.use_optuna = use_optuna\n",
    "        self.n_trials = n_trials\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_and_preprocess_data(self, train_years=['21'], test_year='23', sample_size=5000000):\n",
    "        \"\"\"데이터 로드 및 전처리\"\"\"\n",
    "        print(\"🚀 고도화된 데이터 전처리 시작\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            data_dir = os.path.abspath(os.path.join(base_dir, '..', '데이터'))\n",
    "            \n",
    "            # 훈련 데이터 로드\n",
    "            train_dfs = []\n",
    "            for year in train_years:\n",
    "                file_path = os.path.join(data_dir, f'train_subway{year}.csv')\n",
    "                print(f\"20{year}년 데이터 로드 중...\")\n",
    "                df = pd.read_csv(file_path, encoding='cp949', nrows=sample_size)\n",
    "                df.columns = [col.replace(f'train_subway{year}.', '') for col in df.columns]\n",
    "                train_dfs.append(df)\n",
    "            \n",
    "            self.train_data = pd.concat(train_dfs).reset_index(drop=True)\n",
    "            \n",
    "            # 테스트 데이터 로드\n",
    "            test_file = os.path.join(data_dir, f'train_subway{test_year}.csv')\n",
    "            print(f\"20{test_year}년 검증 데이터 로드 중...\")\n",
    "            self.test_data = pd.read_csv(test_file, encoding='cp949', nrows=sample_size)\n",
    "            self.test_data.columns = [col.replace(f'train_subway{test_year}.', '') for col in self.test_data.columns]\n",
    "            \n",
    "            # 특성 공학 적용\n",
    "            print(\"\\n🔧 고도화된 특성 공학 적용 중...\")\n",
    "            self.train_data = self._apply_feature_engineering(self.train_data)\n",
    "            self.test_data = self._apply_feature_engineering(self.test_data)\n",
    "            \n",
    "            print(f\"\\n✅ 데이터 준비 완료!\")\n",
    "            print(f\"  훈련 데이터: {len(self.train_data):,}개\")\n",
    "            print(f\"  검증 데이터: {len(self.test_data):,}개\")\n",
    "            print(f\"  특성 수: {self.train_data.shape[1]}개\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 데이터 로드 실패: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _apply_feature_engineering(self, df):\n",
    "        \"\"\"특성 공학 파이프라인 적용\"\"\"\n",
    "        print(\"📊 결측치 처리 전 상태:\")\n",
    "        print(f\"  전체 결측치: {df.isnull().sum().sum():,}개\")\n",
    "        \n",
    "        # 0. 기상 데이터 특수 결측치 값 처리\n",
    "        print(\"\\n🔧 기상 데이터 특수 결측치 값 처리...\")\n",
    "        weather_vars = ['ta', 'ws', 'rn_hr1', 'hm','si']\n",
    "        special_missing_values = [-99, -9999, 999, -999, 9999, -88, -77]\n",
    "        \n",
    "        for var in weather_vars:\n",
    "            if var in df.columns:\n",
    "                original_missing = df[var].isnull().sum()\n",
    "                \n",
    "                # 특수 결측치 값들을 NaN으로 변환\n",
    "                for missing_val in special_missing_values:\n",
    "                    special_count = (df[var] == missing_val).sum()\n",
    "                    if special_count > 0:\n",
    "                        print(f\"  {var}: {special_count}개의 {missing_val} 값을 NaN으로 변환\")\n",
    "                        df[var] = df[var].replace(missing_val, np.nan)\n",
    "                \n",
    "                new_missing = df[var].isnull().sum()\n",
    "                if new_missing != original_missing:\n",
    "                    print(f\"  {var}: 결측치 {original_missing} → {new_missing}개\")\n",
    "        \n",
    "        # 비상식적인 값들도 체크 (온도가 -50도 이하나 60도 이상 등)\n",
    "        if 'ta' in df.columns:\n",
    "            extreme_temp = ((df['ta'] < -50) | (df['ta'] > 60)) & df['ta'].notna()\n",
    "            if extreme_temp.sum() > 0:\n",
    "                print(f\"  ta: {extreme_temp.sum()}개의 극한 온도값을 NaN으로 변환\")\n",
    "                df.loc[extreme_temp, 'ta'] = np.nan\n",
    "        \n",
    "        if 'hm' in df.columns:\n",
    "            extreme_hum = ((df['hm'] < 0) | (df['hm'] > 100)) & df['hm'].notna()\n",
    "            if extreme_hum.sum() > 0:\n",
    "                print(f\"  hm: {extreme_hum.sum()}개의 극한 습도값을 NaN으로 변환\")\n",
    "                df.loc[extreme_hum, 'hm'] = np.nan\n",
    "        \n",
    "        if 'ws' in df.columns:\n",
    "            extreme_wind = (df['ws'] < 0) & df['ws'].notna()\n",
    "            if extreme_wind.sum() > 0:\n",
    "                print(f\"  ws: {extreme_wind.sum()}개의 음수 풍속값을 NaN으로 변환\")\n",
    "                df.loc[extreme_wind, 'ws'] = np.nan\n",
    "        \n",
    "        if 'rn_hr1' in df.columns:\n",
    "            extreme_rain = (df['rn_hr1'] < 0) & df['rn_hr1'].notna()\n",
    "            if extreme_rain.sum() > 0:\n",
    "                print(f\"  rn_hr1: {extreme_rain.sum()}개의 음수 강수량을 NaN으로 변환\")\n",
    "                df.loc[extreme_rain, 'rn_hr1'] = np.nan\n",
    "        \n",
    "        print(f\"특수값 처리 후 총 결측치: {df.isnull().sum().sum():,}개\")\n",
    "        \n",
    "        # 1. 극한 기상 특성\n",
    "        df = self.feature_engineer.create_extreme_weather_features(df)\n",
    "        \n",
    "        # 2. 세분화된 시간 특성\n",
    "        df = self.feature_engineer.create_detailed_time_features(df)\n",
    "        \n",
    "        # 3. 상호작용 특성\n",
    "        df = self.feature_engineer.create_interaction_features(df)\n",
    "        \n",
    "        # 4. 시차 특성 (기상 변수만, 혼잡도 시차는 제외)\n",
    "        if 'congestion' in df.columns:\n",
    "            print(\"📈 기상 변수 시차 특성 생성 중 (혼잡도 시차 제외)...\")\n",
    "            \n",
    "            # 시간 정렬\n",
    "            df = df.sort_values(['station_name', 'tm']).reset_index(drop=True)\n",
    "            \n",
    "            # 기상 변수 시차 특성만 생성 (1일 전)\n",
    "            weather_vars = ['ta', 'ws', 'rn_hr1', 'hm']\n",
    "            for var in weather_vars:\n",
    "                if var in df.columns:\n",
    "                    df[f'{var}_lag_24'] = df[var].shift(24)\n",
    "            \n",
    "            # 기상 변수 이동평균 특성 (3,6,12시간)\n",
    "            for var in weather_vars:\n",
    "                if var in df.columns:\n",
    "                    for window in [3, 6, 12]:\n",
    "                        df[f'{var}_ma_{window}'] = df[var].rolling(\n",
    "                            window=window, min_periods=1\n",
    "                        ).mean()\n",
    "            \n",
    "            print(f\"  기상 시차 특성: {len(weather_vars)}개\")\n",
    "            print(f\"  기상 이동평균 특성: {len(weather_vars) * 3}개\")\n",
    "            print(\"  ✅ 혼잡도 과거 데이터는 제외됨\")\n",
    "        \n",
    "        # 5. 체계적인 결측치 처리\n",
    "        print(\"\\n🔧 체계적인 결측치 처리 시작...\")\n",
    "        \n",
    "        # 5-1. 기상 변수 결측치 처리 (시계열 특성 고려)\n",
    "        if 'datetime' in df.columns:\n",
    "            df = df.sort_values(['station_name', 'datetime']).reset_index(drop=True)\n",
    "            \n",
    "            for var in weather_vars:\n",
    "                if var in df.columns:\n",
    "                    missing_before = df[var].isnull().sum()\n",
    "                    if missing_before > 0:\n",
    "                        # Forward fill -> Backward fill -> Median\n",
    "                        df[var] = df.groupby('station_name')[var].fillna(method='ffill').fillna(method='bfill')\n",
    "                        df[var] = df[var].fillna(df[var].median())\n",
    "                        missing_after = df[var].isnull().sum()\n",
    "                        print(f\"  {var}: {missing_before} → {missing_after} 결측치 처리\")\n",
    "        \n",
    "        # 5-2. 시차 특성 결측치 처리 (시계열 특성 특별 처리)\n",
    "        lag_cols = [col for col in df.columns if 'lag_' in col or '_ma_' in col]\n",
    "        for col in lag_cols:\n",
    "            missing_before = df[col].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                # 시차 특성은 0으로 채우거나 기본값 사용\n",
    "                if 'congestion' in col:\n",
    "                    # 혼잡도 시차는 해당 역의 평균값으로\n",
    "                    df[col] = df.groupby('station_name')[col].transform(\n",
    "                        lambda x: x.fillna(x.mean()) if x.notna().any() else x.fillna(50)\n",
    "                    )\n",
    "                else:\n",
    "                    # 기상 시차는 원본 변수 값으로\n",
    "                    base_var = col.split('_lag_')[0] if '_lag_' in col else col.split('_ma_')[0]\n",
    "                    if base_var in df.columns:\n",
    "                        df[col] = df[col].fillna(df[base_var])\n",
    "                    else:\n",
    "                        df[col] = df[col].fillna(0)\n",
    "                \n",
    "                missing_after = df[col].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {col}: {missing_before} → {missing_after} 시차 결측치 처리\")\n",
    "        \n",
    "        # 5-3. 범주형 변수 결측치 처리\n",
    "        categorical_vars = ['time_period', 'detailed_season', 'day_type']\n",
    "        for var in categorical_vars:\n",
    "            if var in df.columns:\n",
    "                missing_before = df[var].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    # 최빈값으로 채우기\n",
    "                    mode_value = df[var].mode()\n",
    "                    if len(mode_value) > 0:\n",
    "                        df[var] = df[var].fillna(mode_value[0])\n",
    "                    missing_after = df[var].isnull().sum()\n",
    "                    print(f\"  {var}: {missing_before} → {missing_after} 범주형 결측치 처리\")\n",
    "        \n",
    "        # 5-4. 이진 변수 결측치 처리 (극한 기상 등)\n",
    "        binary_vars = [col for col in df.columns if col.startswith(('extreme_', 'is_', 'no_', 'light_', 'heavy_', 'strong_', 'calm_'))]\n",
    "        for var in binary_vars:\n",
    "            missing_before = df[var].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df[var] = df[var].fillna(0)  # 이진 변수는 0으로\n",
    "                missing_after = df[var].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {var}: {missing_before} → {missing_after} 이진 결측치 처리\")\n",
    "        \n",
    "        # 5-5. 상호작용 특성 결측치 처리\n",
    "        interaction_vars = [col for col in df.columns if 'interaction' in col or 'apparent_temp' in col or 'wind_chill' in col]\n",
    "        for var in interaction_vars:\n",
    "            missing_before = df[var].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df[var] = df[var].fillna(df[var].median())\n",
    "                missing_after = df[var].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {var}: {missing_before} → {missing_after} 상호작용 결측치 처리\")\n",
    "        \n",
    "        # 5-6. 숫자형 변수 최종 처리 (median)\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_columns:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                missing_before = df[col].isnull().sum()\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "                missing_after = df[col].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {col}: {missing_before} → {missing_after} 기타 숫자형 결측치 처리\")\n",
    "        \n",
    "        # 5-7. 최종 결측치 확인\n",
    "        final_missing = df.isnull().sum().sum()\n",
    "        print(f\"\\n✅ 결측치 처리 완료: {final_missing}개 남음\")\n",
    "        \n",
    "        if final_missing > 0:\n",
    "            print(\"⚠️ 남은 결측치가 있는 컬럼:\")\n",
    "            missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "            for col in missing_cols:\n",
    "                missing_count = df[col].isnull().sum()\n",
    "                missing_pct = missing_count / len(df) * 100\n",
    "                print(f\"  {col}: {missing_count}개 ({missing_pct:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self):\n",
    "        \"\"\"최종 특성 준비\"\"\"\n",
    "        print(\"\\n🎯 최종 특성 준비 중...\")\n",
    "        \n",
    "        # 공통 역 필터링\n",
    "        train_stations = set(self.train_data['station_name'].unique())\n",
    "        test_stations = set(self.test_data['station_name'].unique())\n",
    "        common_stations = train_stations & test_stations\n",
    "        \n",
    "        print(f\"공통 역: {len(common_stations)}개\")\n",
    "        \n",
    "        self.train_data = self.train_data[self.train_data['station_name'].isin(common_stations)]\n",
    "        self.test_data = self.test_data[self.test_data['station_name'].isin(common_stations)]\n",
    "        \n",
    "        # 역 인코딩\n",
    "        le_station = LabelEncoder()\n",
    "        le_station.fit(sorted(common_stations))\n",
    "        self.train_data['station_encoded'] = le_station.transform(self.train_data['station_name'])\n",
    "        self.test_data['station_encoded'] = le_station.transform(self.test_data['station_name'])\n",
    "        \n",
    "        # 범주형 변수 인코딩\n",
    "        categorical_cols = ['time_period', 'detailed_season', 'day_type']\n",
    "        for col in categorical_cols:\n",
    "            if col in self.train_data.columns:\n",
    "                le = LabelEncoder()\n",
    "                # 훈련 데이터와 테스트 데이터의 모든 값으로 fit\n",
    "                combined_values = pd.concat([self.train_data[col], self.test_data[col]]).unique()\n",
    "                le.fit(combined_values)\n",
    "                self.train_data[f'{col}_encoded'] = le.transform(self.train_data[col])\n",
    "                self.test_data[f'{col}_encoded'] = le.transform(self.test_data[col])\n",
    "        \n",
    "        # 명시적으로 제외할 컬럼들 정의\n",
    "        exclude_cols = [\n",
    "            'tm', 'datetime', 'station_name', 'congestion',\n",
    "            'time_period', 'detailed_season', 'day_type'  # 인코딩된 버전을 사용하므로 원본 제외\n",
    "        ]\n",
    "        \n",
    "        # 숫자형 컬럼만 선택 (더 안전한 방법)\n",
    "        numeric_cols = self.train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # 특성 컬럼 선택: 숫자형이면서 제외 목록에 없고, 테스트 데이터에도 있는 컬럼들\n",
    "        feature_cols = [col for col in numeric_cols \n",
    "                       if col not in exclude_cols and col in self.test_data.columns]\n",
    "        \n",
    "        print(f\"전체 숫자형 컬럼: {len(numeric_cols)}개\")\n",
    "        print(f\"제외된 컬럼: {len([col for col in numeric_cols if col in exclude_cols])}개\")\n",
    "        \n",
    "        # 결측치가 많은 특성 제거\n",
    "        missing_threshold = 0.5\n",
    "        features_to_remove = []\n",
    "        for col in feature_cols.copy():\n",
    "            train_missing = self.train_data[col].isnull().mean()\n",
    "            test_missing = self.test_data[col].isnull().mean()\n",
    "            if train_missing > missing_threshold or test_missing > missing_threshold:\n",
    "                features_to_remove.append(col)\n",
    "                feature_cols.remove(col)\n",
    "                print(f\"제거: {col} (훈련 결측치 {train_missing:.1%}, 테스트 결측치 {test_missing:.1%})\")\n",
    "        \n",
    "        # 데이터 타입 확인 및 안전성 검증\n",
    "        print(f\"\\n데이터 타입 검증:\")\n",
    "        for col in feature_cols[:5]:  # 처음 5개만 확인\n",
    "            train_dtype = self.train_data[col].dtype\n",
    "            test_dtype = self.test_data[col].dtype\n",
    "            print(f\"  {col}: 훈련={train_dtype}, 테스트={test_dtype}\")\n",
    "        \n",
    "        # 최종 특성 데이터 생성\n",
    "        X_train = self.train_data[feature_cols].copy()\n",
    "        y_train = self.train_data['congestion'].copy()\n",
    "        X_test = self.test_data[feature_cols].copy()\n",
    "        y_test = self.test_data['congestion'].copy()\n",
    "        \n",
    "        # 문자열이 섞여있는지 최종 확인\n",
    "        for col in feature_cols:\n",
    "            if X_train[col].dtype == 'object':\n",
    "                print(f\"⚠️ 경고: {col}이 문자열 타입입니다. 샘플: {X_train[col].head().tolist()}\")\n",
    "                # 문자열 컬럼이면 제거\n",
    "                feature_cols.remove(col)\n",
    "                X_train = X_train.drop(columns=[col])\n",
    "                X_test = X_test.drop(columns=[col])\n",
    "        \n",
    "        # LightGBM 호환성을 위한 컬럼명 정리\n",
    "        print(f\"\\n🔧 LightGBM 호환성을 위한 컬럼명 정리 중...\")\n",
    "        def clean_feature_name(name):\n",
    "            \"\"\"특수문자를 안전한 문자로 치환\"\"\"\n",
    "            # 특수문자들을 안전한 문자로 치환\n",
    "            replacements = {\n",
    "                '%': 'pct',\n",
    "                '(': '_',\n",
    "                ')': '_',\n",
    "                '[': '_',\n",
    "                ']': '_',\n",
    "                ':': '_',\n",
    "                ' ': '_',\n",
    "                '-': '_',\n",
    "                '/': '_',\n",
    "                '.': '_',\n",
    "                ',': '_',\n",
    "                '&': 'and',\n",
    "                '+': 'plus',\n",
    "                '*': 'mult',\n",
    "                '=': 'eq',\n",
    "                '<': 'lt',\n",
    "                '>': 'gt',\n",
    "                '!': 'not',\n",
    "                '?': 'q',\n",
    "                '@': 'at',\n",
    "                '#': 'hash',\n",
    "                '$': 'dollar'\n",
    "            }\n",
    "            \n",
    "            cleaned_name = name\n",
    "            for old_char, new_char in replacements.items():\n",
    "                cleaned_name = cleaned_name.replace(old_char, new_char)\n",
    "            \n",
    "            # 연속된 언더스코어 정리\n",
    "            while '__' in cleaned_name:\n",
    "                cleaned_name = cleaned_name.replace('__', '_')\n",
    "            \n",
    "            # 시작과 끝의 언더스코어 제거\n",
    "            cleaned_name = cleaned_name.strip('_')\n",
    "            \n",
    "            return cleaned_name\n",
    "        \n",
    "        # 컬럼명 정리 및 변경사항 추적\n",
    "        original_feature_cols = feature_cols.copy()\n",
    "        cleaned_feature_cols = [clean_feature_name(col) for col in feature_cols]\n",
    "        \n",
    "        # 변경된 컬럼명이 있는지 확인\n",
    "        changes_made = False\n",
    "        for original, cleaned in zip(original_feature_cols, cleaned_feature_cols):\n",
    "            if original != cleaned:\n",
    "                if not changes_made:\n",
    "                    print(\"  컬럼명 변경 사항:\")\n",
    "                    changes_made = True\n",
    "                print(f\"    {original} → {cleaned}\")\n",
    "        \n",
    "        if not changes_made:\n",
    "            print(\"  ✅ 모든 컬럼명이 이미 안전함\")\n",
    "        \n",
    "        # DataFrame 컬럼명 변경\n",
    "        column_mapping = dict(zip(original_feature_cols, cleaned_feature_cols))\n",
    "        X_train = X_train.rename(columns=column_mapping)\n",
    "        X_test = X_test.rename(columns=column_mapping)\n",
    "        feature_cols = cleaned_feature_cols\n",
    "        \n",
    "        print(f\"\\n최종 특성 수: {len(feature_cols)}개\")\n",
    "        print(f\"특성 종류: 기본시간, 극한기상, 상호작용, 기상시차, 인코딩\")\n",
    "        print(f\"훈련 데이터 형태: {X_train.shape}\")\n",
    "        print(f\"테스트 데이터 형태: {X_test.shape}\")\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test, feature_cols\n",
    "    \n",
    "    def hyperparameter_tuning(self, X_train, y_train, model_type='xgboost'):\n",
    "        \"\"\"Optuna를 사용한 하이퍼파라미터 튜닝\"\"\"\n",
    "        print(f\"\\n🔍 {model_type} 하이퍼파라미터 튜닝 시작...\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            if model_type == 'xgboost':\n",
    "                params = {\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "                \n",
    "            elif model_type == 'lightgbm':\n",
    "                params = {\n",
    "                    'objective': 'regression',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                    'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': -1\n",
    "                }\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "            \n",
    "            # 시계열 교차검증\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=tscv, \n",
    "                                      scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "            return cv_scores.mean()\n",
    "        \n",
    "        # Optuna 스터디\n",
    "        study = optuna.create_study(direction='maximize', \n",
    "                                  sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"최적 파라미터: {study.best_params}\")\n",
    "        print(f\"최적 CV 점수: {study.best_value:.4f}\")\n",
    "        \n",
    "        return study.best_params\n",
    "    \n",
    "    def feature_selection(self, X_train, y_train, X_test, feature_cols, method='simple'):\n",
    "        \"\"\"빠른 특성 선택\"\"\"\n",
    "        print(f\"\\n🎯 빠른 특성 선택 ({method}) 중...\")\n",
    "        \n",
    "        if method == 'simple' or len(feature_cols) < 20:\n",
    "            # 간단한 방법: 분산이 너무 낮은 특성만 제거\n",
    "            from sklearn.feature_selection import VarianceThreshold\n",
    "            \n",
    "            # 분산 임계값으로 특성 선택 (매우 빠름)\n",
    "            selector = VarianceThreshold(threshold=0.01)\n",
    "            X_train_transformed = selector.fit_transform(X_train)\n",
    "            X_test_transformed = selector.transform(X_test)\n",
    "            \n",
    "            selected_features = [feature_cols[i] for i, selected in enumerate(selector.get_support()) if selected]\n",
    "            \n",
    "            print(f\"분산 기반 선택: {len(selected_features)}개 (전체 {len(feature_cols)}개 중)\")\n",
    "            \n",
    "            X_train_selected = pd.DataFrame(X_train_transformed, columns=selected_features, index=X_train.index)\n",
    "            X_test_selected = pd.DataFrame(X_test_transformed, columns=selected_features, index=X_test.index)\n",
    "            \n",
    "        else:\n",
    "            # 기존 중요도 기반 방법 (더 느림)\n",
    "            rf = RandomForestRegressor(n_estimators=20, random_state=42, n_jobs=1)  # 더 빠르게\n",
    "            rf.fit(X_train, y_train)\n",
    "            \n",
    "            importances = rf.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': feature_cols,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            threshold = 0.005  # 임계값 완화\n",
    "            selected_features = feature_importance_df[\n",
    "                feature_importance_df['importance'] >= threshold\n",
    "            ]['feature'].tolist()\n",
    "            \n",
    "            print(f\"중요도 기반 선택: {len(selected_features)}개\")\n",
    "            \n",
    "            X_train_selected = X_train[selected_features]\n",
    "            X_test_selected = X_test[selected_features]\n",
    "        \n",
    "        self.selected_features = selected_features\n",
    "        return X_train_selected, X_test_selected, selected_features\n",
    "    \n",
    "    def train_enhanced_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"고도화된 모델들 훈련 (XGBoost, LightGBM만)\"\"\"\n",
    "        print(\"\\n🚀 고도화된 모델 훈련 시작\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        models_to_train = {\n",
    "            'Enhanced_XGBoost': 'xgboost',\n",
    "            'Enhanced_LightGBM': 'lightgbm'\n",
    "        }\n",
    "        \n",
    "        for model_name, model_type in models_to_train.items():\n",
    "            print(f\"\\n🔧 {model_name} 훈련 중...\")\n",
    "            \n",
    "            # 하이퍼파라미터 튜닝\n",
    "            if self.use_optuna:\n",
    "                best_params = self.hyperparameter_tuning(X_train, y_train, model_type)\n",
    "            else:\n",
    "                # 기본 파라미터 사용\n",
    "                if model_type == 'xgboost':\n",
    "                    best_params = {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1}\n",
    "                elif model_type == 'lightgbm':\n",
    "                    best_params = {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1}\n",
    "            \n",
    "            # 모델 생성 및 훈련\n",
    "            if model_type == 'xgboost':\n",
    "                model = xgb.XGBRegressor(**best_params)\n",
    "            elif model_type == 'lightgbm':\n",
    "                model = lgb.LGBMRegressor(**best_params)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # 예측 및 평가\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            self.models[model_name] = model\n",
    "            self.results[model_name] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'params': best_params\n",
    "            }\n",
    "            \n",
    "            print(f\"  MAE: {mae:.3f}\")\n",
    "            print(f\"  RMSE: {rmse:.3f}\")\n",
    "            print(f\"  R²: {r2:.3f}\")\n",
    "        \n",
    "        # 최고 성능 모델 선택\n",
    "        best_model_name = min(self.results.keys(), key=lambda x: self.results[x]['mae'])\n",
    "        self.best_model = self.models[best_model_name]\n",
    "        \n",
    "        print(f\"\\n🏆 최고 성능 모델: {best_model_name}\")\n",
    "        print(f\"  MAE: {self.results[best_model_name]['mae']:.3f}\")\n",
    "        print(f\"  R²: {self.results[best_model_name]['r2']:.3f}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def save_model(self, filename='enhanced_subway_model.pkl'):\n",
    "        \"\"\"모델 저장\"\"\"\n",
    "        model_data = {\n",
    "            'best_model': self.best_model,\n",
    "            'feature_engineer': self.feature_engineer,\n",
    "            'selected_features': self.selected_features,\n",
    "            'results': self.results,\n",
    "            'feature_importance': self.feature_importance\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filename)\n",
    "        print(f\"✅ 모델 저장 완료: {filename}\")\n",
    "\n",
    "    def separate_time_effects(self, method='residual'):\n",
    "        \"\"\"시간 효과 분리\"\"\"\n",
    "        print(f\"\\n🕐 시간 효과 분리 시작 ({method} 방법)\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if method == 'residual':\n",
    "            return self._residual_based_separation()\n",
    "        elif method == 'decompose':\n",
    "            return self._seasonal_decompose_separation()\n",
    "        else:\n",
    "            print(\"❌ 지원되지 않는 방법입니다. 'residual' 또는 'decompose'를 사용하세요.\")\n",
    "            return None\n",
    "    \n",
    "    def _residual_based_separation(self):\n",
    "        \"\"\"잔차 기반 시간 효과 분리\"\"\"\n",
    "        print(\"📊 잔차 기반 시간 효과 분리 중...\")\n",
    "        \n",
    "        # 시간 변수만으로 간단 모델 학습\n",
    "        time_features = ['hour', 'dayofweek', 'month', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos']\n",
    "        available_time_features = [f for f in time_features if f in self.train_data.columns]\n",
    "        \n",
    "        print(f\"시간 특성: {available_time_features}\")\n",
    "        \n",
    "        # 시간 모델 학습\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        time_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        \n",
    "        X_time_train = self.train_data[available_time_features]\n",
    "        y_time_train = self.train_data['congestion']\n",
    "        X_time_test = self.test_data[available_time_features]\n",
    "        y_time_test = self.test_data['congestion']\n",
    "        \n",
    "        time_model.fit(X_time_train, y_time_train)\n",
    "        \n",
    "        # 시간 효과 예측\n",
    "        time_pred_train = time_model.predict(X_time_train)\n",
    "        time_pred_test = time_model.predict(X_time_test)\n",
    "        \n",
    "        # 잔차 계산 (시간 효과 제거)\n",
    "        self.train_data['congestion_residual'] = y_time_train - time_pred_train\n",
    "        self.test_data['congestion_residual'] = y_time_test - time_pred_test\n",
    "        \n",
    "        print(f\"시간 모델 R²: {time_model.score(X_time_train, y_time_train):.3f}\")\n",
    "        print(f\"원본 혼잡도 분산: {y_time_train.var():.3f}\")\n",
    "        print(f\"잔차 분산: {self.train_data['congestion_residual'].var():.3f}\")\n",
    "        print(f\"시간 효과 제거율: {(1 - self.train_data['congestion_residual'].var()/y_time_train.var())*100:.1f}%\")\n",
    "        \n",
    "        return time_model\n",
    "    \n",
    "    def _seasonal_decompose_separation(self):\n",
    "        \"\"\"시계열 분해 기반 시간 효과 분리\"\"\"\n",
    "        print(\"📈 시계열 분해 기반 시간 효과 분리 중...\")\n",
    "        \n",
    "        try:\n",
    "            from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "            \n",
    "            # 역별로 시계열 분해\n",
    "            for data_name, data in [('train', self.train_data), ('test', self.test_data)]:\n",
    "                print(f\"{data_name} 데이터 분해 중...\")\n",
    "                \n",
    "                residuals = []\n",
    "                for station in data['station_name'].unique():\n",
    "                    station_data = data[data['station_name'] == station].sort_values('tm')\n",
    "                    \n",
    "                    if len(station_data) >= 48:  # 최소 2일 데이터\n",
    "                        try:\n",
    "                            # 시계열 분해 (24시간 주기)\n",
    "                            decomposition = seasonal_decompose(\n",
    "                                station_data['congestion'], \n",
    "                                model='additive', \n",
    "                                period=24,\n",
    "                                extrapolate_trend='freq'\n",
    "                            )\n",
    "                            \n",
    "                            station_residuals = decomposition.resid.fillna(0)\n",
    "                            residuals.extend(zip(station_data.index, station_residuals))\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ {station} 역 분해 실패: {str(e)}\")\n",
    "                            # 실패 시 원본값 사용\n",
    "                            residuals.extend(zip(station_data.index, station_data['congestion']))\n",
    "                    else:\n",
    "                        # 데이터 부족 시 원본값 사용  \n",
    "                        residuals.extend(zip(station_data.index, station_data['congestion']))\n",
    "                \n",
    "                # 잔차 설정\n",
    "                residual_dict = dict(residuals)\n",
    "                data['congestion_residual'] = data.index.map(residual_dict).fillna(data['congestion'])\n",
    "                \n",
    "                print(f\"{data_name} 원본 분산: {data['congestion'].var():.3f}\")\n",
    "                print(f\"{data_name} 잔차 분산: {data['congestion_residual'].var():.3f}\")\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"⚠️ statsmodels 미설치. pip install statsmodels\")\n",
    "            return self._residual_based_separation()  # 대안 사용\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"메인 실행 함수 - 시간 효과 분리 포함\"\"\"\n",
    "    print(\"🚀 고도화된 지하철 혼잡도 예측 모델 (시간효과분리)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = EnhancedSubwayModel(use_optuna=True, n_trials=10)\n",
    "    \n",
    "    # 1. 데이터 로드 및 전처리\n",
    "    if not model.load_and_preprocess_data(train_years=['21'], test_year='23', sample_size=5000000):\n",
    "        return None\n",
    "    \n",
    "    # 2. 시간 효과 분리 (옵션)\n",
    "    use_time_separation = True  # 시간 효과 분리 사용 여부\n",
    "    separation_method = 'residual'  # 'residual' 또는 'decompose'\n",
    "    \n",
    "    if use_time_separation:\n",
    "        model.separate_time_effects(method=separation_method)\n",
    "        target_col = 'congestion_residual'  # 잔차를 타겟으로 사용\n",
    "        print(f\"✅ 타겟 변수: {target_col} (시간 효과 제거됨)\")\n",
    "    else:\n",
    "        target_col = 'congestion'  # 원본 사용\n",
    "        print(f\"✅ 타겟 변수: {target_col} (원본)\")\n",
    "    \n",
    "    # 3. 특성 준비 (시간 효과 분리된 타겟 사용)\n",
    "    original_congestion = model.train_data['congestion'].copy()\n",
    "    model.train_data['congestion'] = model.train_data[target_col]\n",
    "    model.test_data['congestion'] = model.test_data[target_col]\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, feature_cols = model.prepare_features()\n",
    "    \n",
    "    # 4. 특성 선택\n",
    "    X_train_selected, X_test_selected, selected_features = model.feature_selection(\n",
    "        X_train, y_train, X_test, feature_cols, method='simple'\n",
    "    )\n",
    "    \n",
    "    # 5. 모델 훈련\n",
    "    results = model.train_enhanced_models(X_train_selected, y_train, X_test_selected, y_test)\n",
    "    \n",
    "    # 6. 모델 저장\n",
    "    try:\n",
    "        filename = f'enhanced_model_{\"time_separated\" if use_time_separation else \"original\"}.pkl'\n",
    "        model.save_model(filename)\n",
    "    except:\n",
    "        model.save_model('enhanced_model_time_separated.pkl')\n",
    "    \n",
    "    print(f\"\\n🎉 모델 학습 완료!\")\n",
    "    return model, X_test_selected, y_test, selected_features, results, use_time_separation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 실행\n",
    "model, X_test, y_test, selected_features, results, use_time_separation = train_model()\n",
    "\n",
    "# 분석용 데이터 저장\n",
    "import pickle\n",
    "analysis_data = {\n",
    "    'model': model,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test, \n",
    "    'selected_features': selected_features,\n",
    "    'results': results,\n",
    "    'use_time_separation': use_time_separation,\n",
    "    'train_data': model.train_data,\n",
    "    'test_data': model.test_data\n",
    "}\n",
    "\n",
    "with open('model_data_for_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(analysis_data, f)\n",
    "\n",
    "print(\"✅ 분석용 데이터 저장 완료: model_data_for_analysis.pkl\")\n",
    "print(f\"저장된 모델 수: {len(results)}\")\n",
    "print(f\"테스트 샘플 수: {len(y_test):,}\")\n",
    "print(f\"선택된 특성 수: {len(selected_features)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
