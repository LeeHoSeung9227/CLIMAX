{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFeatureEngineering:\n",
    "    \"\"\"ê³ ë„í™”ëœ íŠ¹ì„± ê³µí•™\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        self.selected_features = None\n",
    "        \n",
    "    def create_extreme_weather_features(self, df):\n",
    "        \"\"\"ê·¹í•œ ê¸°ìƒ ìƒíƒœ ì´ì§„ ë³€ìˆ˜ ìƒì„±\"\"\"\n",
    "        print(\"ğŸŒªï¸ ê·¹í•œ ê¸°ìƒ ìƒíƒœ ë³€ìˆ˜ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ì˜¨ë„ ê¸°ë°˜ ê·¹í•œ ìƒíƒœ\n",
    "        if 'ta' in df.columns:\n",
    "            temp_q05 = df['ta'].quantile(0.05)  # í•œíŒŒ\n",
    "            temp_q95 = df['ta'].quantile(0.95)  # í­ì—¼\n",
    "            \n",
    "            df['extreme_cold'] = (df['ta'] <= temp_q05).astype(int)\n",
    "            df['extreme_heat'] = (df['ta'] >= temp_q95).astype(int)\n",
    "            df['moderate_temp'] = ((df['ta'] > temp_q05) & (df['ta'] < temp_q95)).astype(int)\n",
    "            \n",
    "            print(f\"  í•œíŒŒ ê¸°ì¤€: â‰¤{temp_q05:.1f}Â°C ({df['extreme_cold'].sum():,}ê°œ)\")\n",
    "            print(f\"  í­ì—¼ ê¸°ì¤€: â‰¥{temp_q95:.1f}Â°C ({df['extreme_heat'].sum():,}ê°œ)\")\n",
    "        \n",
    "        # ê°•ìˆ˜ ê¸°ë°˜ ê·¹í•œ ìƒíƒœ\n",
    "        if 'rn_hr1' in df.columns:\n",
    "            df['no_rain'] = (df['rn_hr1'] == 0).astype(int)\n",
    "            df['light_rain'] = ((df['rn_hr1'] > 0) & (df['rn_hr1'] <= 2)).astype(int)\n",
    "            df['heavy_rain'] = (df['rn_hr1'] > 10).astype(int)\n",
    "            df['extreme_rain'] = (df['rn_hr1'] > 30).astype(int)\n",
    "            \n",
    "            print(f\"  í­ìš° ê¸°ì¤€: >10mm ({df['heavy_rain'].sum():,}ê°œ)\")\n",
    "            print(f\"  ê·¹í•œ ê°•ìˆ˜: >30mm ({df['extreme_rain'].sum():,}ê°œ)\")\n",
    "        \n",
    "        # í’ì† ê¸°ë°˜ ê·¹í•œ ìƒíƒœ\n",
    "        if 'ws' in df.columns:\n",
    "            wind_q90 = df['ws'].quantile(0.90)\n",
    "            wind_q95 = df['ws'].quantile(0.95)\n",
    "            \n",
    "            df['calm_wind'] = (df['ws'] <= 1.0).astype(int)\n",
    "            df['strong_wind'] = (df['ws'] >= wind_q90).astype(int)\n",
    "            df['extreme_wind'] = (df['ws'] >= wind_q95).astype(int)\n",
    "            \n",
    "            print(f\"  ê°•í’ ê¸°ì¤€: â‰¥{wind_q90:.1f}m/s ({df['strong_wind'].sum():,}ê°œ)\")\n",
    "            print(f\"  ê·¹í•œ í’ì†: â‰¥{wind_q95:.1f}m/s ({df['extreme_wind'].sum():,}ê°œ)\")\n",
    "        \n",
    "        # ìŠµë„ ê¸°ë°˜ ê·¹í•œ ìƒíƒœ\n",
    "        if 'hm' in df.columns:\n",
    "            humidity_q05 = df['hm'].quantile(0.05)\n",
    "            humidity_q95 = df['hm'].quantile(0.95)\n",
    "            \n",
    "            df['extreme_dry'] = (df['hm'] <= humidity_q05).astype(int)\n",
    "            df['extreme_humid'] = (df['hm'] >= humidity_q95).astype(int)\n",
    "            \n",
    "            print(f\"  ê·¹ê±´ì¡°: â‰¤{humidity_q05:.1f}% ({df['extreme_dry'].sum():,}ê°œ)\")\n",
    "            print(f\"  ê·¹ìŠµí•¨: â‰¥{humidity_q95:.1f}% ({df['extreme_humid'].sum():,}ê°œ)\")\n",
    "        \n",
    "        # ë³µí•© ê·¹í•œ ìƒíƒœ\n",
    "        df['extreme_weather_any'] = (\n",
    "            df.get('extreme_cold', 0) | df.get('extreme_heat', 0) |\n",
    "            df.get('heavy_rain', 0) | df.get('extreme_wind', 0) |\n",
    "            df.get('extreme_dry', 0) | df.get('extreme_humid', 0)\n",
    "        ).astype(int)\n",
    "        \n",
    "        print(f\"  ì „ì²´ ê·¹í•œ ê¸°ìƒ: {df['extreme_weather_any'].sum():,}ê°œ ({df['extreme_weather_any'].mean()*100:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_detailed_time_features(self, df):\n",
    "        \"\"\"ì„¸ë¶„í™”ëœ ì‹œê°„ëŒ€/ê³„ì ˆ ë²”ì£¼í˜• ë³€ìˆ˜\"\"\"\n",
    "        print(\"â° ì„¸ë¶„í™”ëœ ì‹œê°„ íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ì‹œê°„ ê´€ë ¨ ê¸°ë³¸ íŠ¹ì„±\n",
    "        df['datetime'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        \n",
    "        # ì„¸ë¶„í™”ëœ ì¶œí‡´ê·¼ ì‹œê°„ëŒ€\n",
    "        def get_detailed_time_period(hour):\n",
    "            if hour in [6, 7]:\n",
    "                return 'early_morning_rush'\n",
    "            elif hour in [8, 9]:\n",
    "                return 'morning_rush_peak'\n",
    "            elif hour == 10:\n",
    "                return 'morning_rush_end'\n",
    "            elif hour in [11, 12, 13, 14]:\n",
    "                return 'daytime'\n",
    "            elif hour in [15, 16]:\n",
    "                return 'afternoon_start'\n",
    "            elif hour in [17, 18]:\n",
    "                return 'evening_rush_start'\n",
    "            elif hour in [19, 20]:\n",
    "                return 'evening_rush_peak'\n",
    "            elif hour == 21:\n",
    "                return 'evening_rush_end'\n",
    "            elif hour in [22, 23]:\n",
    "                return 'night'\n",
    "            else:  # 0-5ì‹œ\n",
    "                return 'late_night'\n",
    "        \n",
    "        df['time_period'] = df['hour'].apply(get_detailed_time_period)\n",
    "        \n",
    "        # ì„¸ë¶„í™”ëœ ê³„ì ˆ\n",
    "        def get_detailed_season(month):\n",
    "            if month in [12, 1, 2]:\n",
    "                return 'winter'\n",
    "            elif month in [3, 4]:\n",
    "                return 'spring_early'\n",
    "            elif month == 5:\n",
    "                return 'spring_late'\n",
    "            elif month in [6, 7]:\n",
    "                return 'summer_early'\n",
    "            elif month == 8:\n",
    "                return 'summer_peak'\n",
    "            elif month in [9, 10]:\n",
    "                return 'autumn_early'\n",
    "            else:  # 11ì›”\n",
    "                return 'autumn_late'\n",
    "        \n",
    "        df['detailed_season'] = df['month'].apply(get_detailed_season)\n",
    "        \n",
    "        # ì£¼ë§/í‰ì¼ ì„¸ë¶„í™”\n",
    "        df['day_type'] = df['dayofweek'].apply(\n",
    "            lambda x: 'weekend' if x >= 5 else 'weekday'\n",
    "        )\n",
    "        \n",
    "        # ì›”ìš”ì¼/ê¸ˆìš”ì¼ íš¨ê³¼\n",
    "        df['is_monday'] = (df['dayofweek'] == 0).astype(int)\n",
    "        df['is_friday'] = (df['dayofweek'] == 4).astype(int)\n",
    "        \n",
    "        # ìˆœí™˜ì  ì‹œê°„ íŠ¹ì„± (ê¸°ì¡´ + ì¶”ê°€)\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "        df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "        \n",
    "        print(f\"  ì„¸ë¶„í™”ëœ ì‹œê°„ëŒ€: {df['time_period'].nunique()}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "        print(f\"  ì„¸ë¶„í™”ëœ ê³„ì ˆ: {df['detailed_season'].nunique()}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_interaction_features(self, df):\n",
    "        \"\"\"ìƒí˜¸ì‘ìš© í•­ ìƒì„±\"\"\"\n",
    "        print(\"ğŸ”— ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ê¸°ìƒ ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš©\n",
    "        if 'ta' in df.columns and 'hm' in df.columns:\n",
    "            # ì²´ê°ì˜¨ë„ (ì˜¨ë„ Ã— ìŠµë„)\n",
    "            df['apparent_temp'] = df['ta'] * (1 + df['hm'] / 100)\n",
    "            df['temp_humidity_interaction'] = df['ta'] * df['hm']\n",
    "            print(\"  ì˜¨ë„ Ã— ìŠµë„ ìƒí˜¸ì‘ìš© ìƒì„±\")\n",
    "        \n",
    "        if 'ta' in df.columns and 'ws' in df.columns:\n",
    "            # í’ì†ì— ì˜í•œ ì²´ê°ì˜¨ë„\n",
    "            df['wind_chill'] = df['ta'] - df['ws'] * 2\n",
    "            df['temp_wind_interaction'] = df['ta'] * df['ws']\n",
    "            print(\"  ì˜¨ë„ Ã— í’ì† ìƒí˜¸ì‘ìš© ìƒì„±\")\n",
    "        \n",
    "        if 'rn_hr1' in df.columns and 'ws' in df.columns:\n",
    "            # ë¹„ë°”ëŒ íš¨ê³¼\n",
    "            df['rain_wind_interaction'] = df['rn_hr1'] * df['ws']\n",
    "            print(\"  ê°•ìˆ˜ Ã— í’ì† ìƒí˜¸ì‘ìš© ìƒì„±\")\n",
    "        \n",
    "        if 'rn_hr1' in df.columns and 'hm' in df.columns:\n",
    "            # ìŠµë„-ê°•ìˆ˜ ìƒí˜¸ì‘ìš©\n",
    "            df['rain_humidity_interaction'] = df['rn_hr1'] * df['hm']\n",
    "            print(\"  ê°•ìˆ˜ Ã— ìŠµë„ ìƒí˜¸ì‘ìš© ìƒì„±\")\n",
    "        \n",
    "        # ì‹œê°„-ê¸°ìƒ ìƒí˜¸ì‘ìš©\n",
    "        if 'ta' in df.columns:\n",
    "            df['temp_hour_interaction'] = df['ta'] * df['hour']\n",
    "            df['temp_season_interaction'] = df['ta'] * df['month']\n",
    "            print(\"  ì˜¨ë„ Ã— ì‹œê°„ ìƒí˜¸ì‘ìš© ìƒì„±\")\n",
    "        \n",
    "        # ê·¹í•œ ê¸°ìƒê³¼ ì‹œê°„ ìƒí˜¸ì‘ìš©\n",
    "        if 'extreme_weather_any' in df.columns:\n",
    "            df['extreme_weather_rush'] = df['extreme_weather_any'] * (\n",
    "                df['time_period'].isin(['morning_rush_peak', 'evening_rush_peak']).astype(int)\n",
    "            )\n",
    "            print(\"  ê·¹í•œê¸°ìƒ Ã— ì¶œí‡´ê·¼ì‹œê°„ ìƒí˜¸ì‘ìš© ìƒì„±\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_lag_features(self, df, target_col='congestion'):\n",
    "        \"\"\"ì‹œì°¨ íŠ¹ì„± ìƒì„±\"\"\"\n",
    "        print(\"ğŸ“ˆ ê¸°ìƒ ë³€ìˆ˜ ì‹œì°¨ íŠ¹ì„± ìƒì„± ì¤‘ (í˜¼ì¡ë„ ì‹œì°¨ ì œì™¸)...\")\n",
    "        \n",
    "        # ì‹œê°„ ì •ë ¬\n",
    "        df = df.sort_values(['station_name', 'tm']).reset_index(drop=True)\n",
    "        \n",
    "        # ê¸°ìƒ ë³€ìˆ˜ ì‹œì°¨ íŠ¹ì„±ë§Œ ìƒì„± (1ì¼ ì „)\n",
    "        weather_vars = ['ta', 'ws', 'rn_hr1', 'hm']\n",
    "        for var in weather_vars:\n",
    "            if var in df.columns:\n",
    "                df[f'{var}_lag_24'] = df[var].shift(24)\n",
    "        \n",
    "        # ê¸°ìƒ ë³€ìˆ˜ ì´ë™í‰ê·  íŠ¹ì„± (3,6,12ì‹œê°„)\n",
    "        for var in weather_vars:\n",
    "            if var in df.columns:\n",
    "                for window in [3, 6, 12]:\n",
    "                    df[f'{var}_ma_{window}'] = df[var].rolling(\n",
    "                        window=window, min_periods=1\n",
    "                    ).mean()\n",
    "        \n",
    "        print(f\"  ê¸°ìƒ ì‹œì°¨ íŠ¹ì„±: {len(weather_vars)}ê°œ\")\n",
    "        print(f\"  ê¸°ìƒ ì´ë™í‰ê·  íŠ¹ì„±: {len(weather_vars) * 3}ê°œ\")\n",
    "        print(\"  âœ… í˜¼ì¡ë„ ê³¼ê±° ë°ì´í„°ëŠ” ì œì™¸ë¨\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSubwayModel:\n",
    "    \"\"\"ê³ ë„í™”ëœ ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(self, use_optuna=True, n_trials=100):\n",
    "        self.feature_engineer = EnhancedFeatureEngineering()\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.feature_importance = None\n",
    "        self.use_optuna = use_optuna\n",
    "        self.n_trials = n_trials\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_and_preprocess_data(self, train_years=['21'], test_year='23', sample_size=5000000):\n",
    "        \"\"\"ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\"\"\"\n",
    "        print(\"ğŸš€ ê³ ë„í™”ëœ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            data_dir = os.path.abspath(os.path.join(base_dir, '..', 'ë°ì´í„°'))\n",
    "            \n",
    "            # í›ˆë ¨ ë°ì´í„° ë¡œë“œ\n",
    "            train_dfs = []\n",
    "            for year in train_years:\n",
    "                file_path = os.path.join(data_dir, f'train_subway{year}.csv')\n",
    "                print(f\"20{year}ë…„ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "                df = pd.read_csv(file_path, encoding='cp949', nrows=sample_size)\n",
    "                df.columns = [col.replace(f'train_subway{year}.', '') for col in df.columns]\n",
    "                train_dfs.append(df)\n",
    "            \n",
    "            self.train_data = pd.concat(train_dfs).reset_index(drop=True)\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "            test_file = os.path.join(data_dir, f'train_subway{test_year}.csv')\n",
    "            print(f\"20{test_year}ë…„ ê²€ì¦ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "            self.test_data = pd.read_csv(test_file, encoding='cp949', nrows=sample_size)\n",
    "            self.test_data.columns = [col.replace(f'train_subway{test_year}.', '') for col in self.test_data.columns]\n",
    "            \n",
    "            # íŠ¹ì„± ê³µí•™ ì ìš©\n",
    "            print(\"\\nğŸ”§ ê³ ë„í™”ëœ íŠ¹ì„± ê³µí•™ ì ìš© ì¤‘...\")\n",
    "            self.train_data = self._apply_feature_engineering(self.train_data)\n",
    "            self.test_data = self._apply_feature_engineering(self.test_data)\n",
    "            \n",
    "            print(f\"\\nâœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "            print(f\"  í›ˆë ¨ ë°ì´í„°: {len(self.train_data):,}ê°œ\")\n",
    "            print(f\"  ê²€ì¦ ë°ì´í„°: {len(self.test_data):,}ê°œ\")\n",
    "            print(f\"  íŠ¹ì„± ìˆ˜: {self.train_data.shape[1]}ê°œ\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _apply_feature_engineering(self, df):\n",
    "        \"\"\"íŠ¹ì„± ê³µí•™ íŒŒì´í”„ë¼ì¸ ì ìš©\"\"\"\n",
    "        print(\"ğŸ“Š ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ ìƒíƒœ:\")\n",
    "        print(f\"  ì „ì²´ ê²°ì¸¡ì¹˜: {df.isnull().sum().sum():,}ê°œ\")\n",
    "        \n",
    "        # 0. ê¸°ìƒ ë°ì´í„° íŠ¹ìˆ˜ ê²°ì¸¡ì¹˜ ê°’ ì²˜ë¦¬\n",
    "        print(\"\\nğŸ”§ ê¸°ìƒ ë°ì´í„° íŠ¹ìˆ˜ ê²°ì¸¡ì¹˜ ê°’ ì²˜ë¦¬...\")\n",
    "        weather_vars = ['ta', 'ws', 'rn_hr1', 'hm','si']\n",
    "        special_missing_values = [-99, -9999, 999, -999, 9999, -88, -77]\n",
    "        \n",
    "        for var in weather_vars:\n",
    "            if var in df.columns:\n",
    "                original_missing = df[var].isnull().sum()\n",
    "                \n",
    "                # íŠ¹ìˆ˜ ê²°ì¸¡ì¹˜ ê°’ë“¤ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "                for missing_val in special_missing_values:\n",
    "                    special_count = (df[var] == missing_val).sum()\n",
    "                    if special_count > 0:\n",
    "                        print(f\"  {var}: {special_count}ê°œì˜ {missing_val} ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\")\n",
    "                        df[var] = df[var].replace(missing_val, np.nan)\n",
    "                \n",
    "                new_missing = df[var].isnull().sum()\n",
    "                if new_missing != original_missing:\n",
    "                    print(f\"  {var}: ê²°ì¸¡ì¹˜ {original_missing} â†’ {new_missing}ê°œ\")\n",
    "        \n",
    "        # ë¹„ìƒì‹ì ì¸ ê°’ë“¤ë„ ì²´í¬ (ì˜¨ë„ê°€ -50ë„ ì´í•˜ë‚˜ 60ë„ ì´ìƒ ë“±)\n",
    "        if 'ta' in df.columns:\n",
    "            extreme_temp = ((df['ta'] < -50) | (df['ta'] > 60)) & df['ta'].notna()\n",
    "            if extreme_temp.sum() > 0:\n",
    "                print(f\"  ta: {extreme_temp.sum()}ê°œì˜ ê·¹í•œ ì˜¨ë„ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\")\n",
    "                df.loc[extreme_temp, 'ta'] = np.nan\n",
    "        \n",
    "        if 'hm' in df.columns:\n",
    "            extreme_hum = ((df['hm'] < 0) | (df['hm'] > 100)) & df['hm'].notna()\n",
    "            if extreme_hum.sum() > 0:\n",
    "                print(f\"  hm: {extreme_hum.sum()}ê°œì˜ ê·¹í•œ ìŠµë„ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\")\n",
    "                df.loc[extreme_hum, 'hm'] = np.nan\n",
    "        \n",
    "        if 'ws' in df.columns:\n",
    "            extreme_wind = (df['ws'] < 0) & df['ws'].notna()\n",
    "            if extreme_wind.sum() > 0:\n",
    "                print(f\"  ws: {extreme_wind.sum()}ê°œì˜ ìŒìˆ˜ í’ì†ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\")\n",
    "                df.loc[extreme_wind, 'ws'] = np.nan\n",
    "        \n",
    "        if 'rn_hr1' in df.columns:\n",
    "            extreme_rain = (df['rn_hr1'] < 0) & df['rn_hr1'].notna()\n",
    "            if extreme_rain.sum() > 0:\n",
    "                print(f\"  rn_hr1: {extreme_rain.sum()}ê°œì˜ ìŒìˆ˜ ê°•ìˆ˜ëŸ‰ì„ NaNìœ¼ë¡œ ë³€í™˜\")\n",
    "                df.loc[extreme_rain, 'rn_hr1'] = np.nan\n",
    "        \n",
    "        print(f\"íŠ¹ìˆ˜ê°’ ì²˜ë¦¬ í›„ ì´ ê²°ì¸¡ì¹˜: {df.isnull().sum().sum():,}ê°œ\")\n",
    "        \n",
    "        # 1. ê·¹í•œ ê¸°ìƒ íŠ¹ì„±\n",
    "        df = self.feature_engineer.create_extreme_weather_features(df)\n",
    "        \n",
    "        # 2. ì„¸ë¶„í™”ëœ ì‹œê°„ íŠ¹ì„±\n",
    "        df = self.feature_engineer.create_detailed_time_features(df)\n",
    "        \n",
    "        # 3. ìƒí˜¸ì‘ìš© íŠ¹ì„±\n",
    "        df = self.feature_engineer.create_interaction_features(df)\n",
    "        \n",
    "        # 4. ì‹œì°¨ íŠ¹ì„± (ê¸°ìƒ ë³€ìˆ˜ë§Œ, í˜¼ì¡ë„ ì‹œì°¨ëŠ” ì œì™¸)\n",
    "        if 'congestion' in df.columns:\n",
    "            print(\"ğŸ“ˆ ê¸°ìƒ ë³€ìˆ˜ ì‹œì°¨ íŠ¹ì„± ìƒì„± ì¤‘ (í˜¼ì¡ë„ ì‹œì°¨ ì œì™¸)...\")\n",
    "            \n",
    "            # ì‹œê°„ ì •ë ¬\n",
    "            df = df.sort_values(['station_name', 'tm']).reset_index(drop=True)\n",
    "            \n",
    "            # ê¸°ìƒ ë³€ìˆ˜ ì‹œì°¨ íŠ¹ì„±ë§Œ ìƒì„± (1ì¼ ì „)\n",
    "            weather_vars = ['ta', 'ws', 'rn_hr1', 'hm']\n",
    "            for var in weather_vars:\n",
    "                if var in df.columns:\n",
    "                    df[f'{var}_lag_24'] = df[var].shift(24)\n",
    "            \n",
    "            # ê¸°ìƒ ë³€ìˆ˜ ì´ë™í‰ê·  íŠ¹ì„± (3,6,12ì‹œê°„)\n",
    "            for var in weather_vars:\n",
    "                if var in df.columns:\n",
    "                    for window in [3, 6, 12]:\n",
    "                        df[f'{var}_ma_{window}'] = df[var].rolling(\n",
    "                            window=window, min_periods=1\n",
    "                        ).mean()\n",
    "            \n",
    "            print(f\"  ê¸°ìƒ ì‹œì°¨ íŠ¹ì„±: {len(weather_vars)}ê°œ\")\n",
    "            print(f\"  ê¸°ìƒ ì´ë™í‰ê·  íŠ¹ì„±: {len(weather_vars) * 3}ê°œ\")\n",
    "            print(\"  âœ… í˜¼ì¡ë„ ê³¼ê±° ë°ì´í„°ëŠ” ì œì™¸ë¨\")\n",
    "        \n",
    "        # 5. ì²´ê³„ì ì¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        print(\"\\nğŸ”§ ì²´ê³„ì ì¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        \n",
    "        # 5-1. ê¸°ìƒ ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì‹œê³„ì—´ íŠ¹ì„± ê³ ë ¤)\n",
    "        if 'datetime' in df.columns:\n",
    "            df = df.sort_values(['station_name', 'datetime']).reset_index(drop=True)\n",
    "            \n",
    "            for var in weather_vars:\n",
    "                if var in df.columns:\n",
    "                    missing_before = df[var].isnull().sum()\n",
    "                    if missing_before > 0:\n",
    "                        # Forward fill -> Backward fill -> Median\n",
    "                        df[var] = df.groupby('station_name')[var].fillna(method='ffill').fillna(method='bfill')\n",
    "                        df[var] = df[var].fillna(df[var].median())\n",
    "                        missing_after = df[var].isnull().sum()\n",
    "                        print(f\"  {var}: {missing_before} â†’ {missing_after} ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
    "        \n",
    "        # 5-2. ì‹œì°¨ íŠ¹ì„± ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì‹œê³„ì—´ íŠ¹ì„± íŠ¹ë³„ ì²˜ë¦¬)\n",
    "        lag_cols = [col for col in df.columns if 'lag_' in col or '_ma_' in col]\n",
    "        for col in lag_cols:\n",
    "            missing_before = df[col].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                # ì‹œì°¨ íŠ¹ì„±ì€ 0ìœ¼ë¡œ ì±„ìš°ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "                if 'congestion' in col:\n",
    "                    # í˜¼ì¡ë„ ì‹œì°¨ëŠ” í•´ë‹¹ ì—­ì˜ í‰ê· ê°’ìœ¼ë¡œ\n",
    "                    df[col] = df.groupby('station_name')[col].transform(\n",
    "                        lambda x: x.fillna(x.mean()) if x.notna().any() else x.fillna(50)\n",
    "                    )\n",
    "                else:\n",
    "                    # ê¸°ìƒ ì‹œì°¨ëŠ” ì›ë³¸ ë³€ìˆ˜ ê°’ìœ¼ë¡œ\n",
    "                    base_var = col.split('_lag_')[0] if '_lag_' in col else col.split('_ma_')[0]\n",
    "                    if base_var in df.columns:\n",
    "                        df[col] = df[col].fillna(df[base_var])\n",
    "                    else:\n",
    "                        df[col] = df[col].fillna(0)\n",
    "                \n",
    "                missing_after = df[col].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {col}: {missing_before} â†’ {missing_after} ì‹œì°¨ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
    "        \n",
    "        # 5-3. ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        categorical_vars = ['time_period', 'detailed_season', 'day_type']\n",
    "        for var in categorical_vars:\n",
    "            if var in df.columns:\n",
    "                missing_before = df[var].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    # ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "                    mode_value = df[var].mode()\n",
    "                    if len(mode_value) > 0:\n",
    "                        df[var] = df[var].fillna(mode_value[0])\n",
    "                    missing_after = df[var].isnull().sum()\n",
    "                    print(f\"  {var}: {missing_before} â†’ {missing_after} ë²”ì£¼í˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
    "        \n",
    "        # 5-4. ì´ì§„ ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ê·¹í•œ ê¸°ìƒ ë“±)\n",
    "        binary_vars = [col for col in df.columns if col.startswith(('extreme_', 'is_', 'no_', 'light_', 'heavy_', 'strong_', 'calm_'))]\n",
    "        for var in binary_vars:\n",
    "            missing_before = df[var].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df[var] = df[var].fillna(0)  # ì´ì§„ ë³€ìˆ˜ëŠ” 0ìœ¼ë¡œ\n",
    "                missing_after = df[var].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {var}: {missing_before} â†’ {missing_after} ì´ì§„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
    "        \n",
    "        # 5-5. ìƒí˜¸ì‘ìš© íŠ¹ì„± ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        interaction_vars = [col for col in df.columns if 'interaction' in col or 'apparent_temp' in col or 'wind_chill' in col]\n",
    "        for var in interaction_vars:\n",
    "            missing_before = df[var].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df[var] = df[var].fillna(df[var].median())\n",
    "                missing_after = df[var].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {var}: {missing_before} â†’ {missing_after} ìƒí˜¸ì‘ìš© ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
    "        \n",
    "        # 5-6. ìˆ«ìí˜• ë³€ìˆ˜ ìµœì¢… ì²˜ë¦¬ (median)\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_columns:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                missing_before = df[col].isnull().sum()\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "                missing_after = df[col].isnull().sum()\n",
    "                if missing_before > 0:\n",
    "                    print(f\"  {col}: {missing_before} â†’ {missing_after} ê¸°íƒ€ ìˆ«ìí˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
    "        \n",
    "        # 5-7. ìµœì¢… ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "        final_missing = df.isnull().sum().sum()\n",
    "        print(f\"\\nâœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {final_missing}ê°œ ë‚¨ìŒ\")\n",
    "        \n",
    "        if final_missing > 0:\n",
    "            print(\"âš ï¸ ë‚¨ì€ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼:\")\n",
    "            missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "            for col in missing_cols:\n",
    "                missing_count = df[col].isnull().sum()\n",
    "                missing_pct = missing_count / len(df) * 100\n",
    "                print(f\"  {col}: {missing_count}ê°œ ({missing_pct:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self):\n",
    "        \"\"\"ìµœì¢… íŠ¹ì„± ì¤€ë¹„\"\"\"\n",
    "        print(\"\\nğŸ¯ ìµœì¢… íŠ¹ì„± ì¤€ë¹„ ì¤‘...\")\n",
    "        \n",
    "        # ê³µí†µ ì—­ í•„í„°ë§\n",
    "        train_stations = set(self.train_data['station_name'].unique())\n",
    "        test_stations = set(self.test_data['station_name'].unique())\n",
    "        common_stations = train_stations & test_stations\n",
    "        \n",
    "        print(f\"ê³µí†µ ì—­: {len(common_stations)}ê°œ\")\n",
    "        \n",
    "        self.train_data = self.train_data[self.train_data['station_name'].isin(common_stations)]\n",
    "        self.test_data = self.test_data[self.test_data['station_name'].isin(common_stations)]\n",
    "        \n",
    "        # ì—­ ì¸ì½”ë”©\n",
    "        le_station = LabelEncoder()\n",
    "        le_station.fit(sorted(common_stations))\n",
    "        self.train_data['station_encoded'] = le_station.transform(self.train_data['station_name'])\n",
    "        self.test_data['station_encoded'] = le_station.transform(self.test_data['station_name'])\n",
    "        \n",
    "        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "        categorical_cols = ['time_period', 'detailed_season', 'day_type']\n",
    "        for col in categorical_cols:\n",
    "            if col in self.train_data.columns:\n",
    "                le = LabelEncoder()\n",
    "                # í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ë“  ê°’ìœ¼ë¡œ fit\n",
    "                combined_values = pd.concat([self.train_data[col], self.test_data[col]]).unique()\n",
    "                le.fit(combined_values)\n",
    "                self.train_data[f'{col}_encoded'] = le.transform(self.train_data[col])\n",
    "                self.test_data[f'{col}_encoded'] = le.transform(self.test_data[col])\n",
    "        \n",
    "        # ëª…ì‹œì ìœ¼ë¡œ ì œì™¸í•  ì»¬ëŸ¼ë“¤ ì •ì˜\n",
    "        exclude_cols = [\n",
    "            'tm', 'datetime', 'station_name', 'congestion',\n",
    "            'time_period', 'detailed_season', 'day_type'  # ì¸ì½”ë”©ëœ ë²„ì „ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì›ë³¸ ì œì™¸\n",
    "        ]\n",
    "        \n",
    "        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ (ë” ì•ˆì „í•œ ë°©ë²•)\n",
    "        numeric_cols = self.train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ: ìˆ«ìí˜•ì´ë©´ì„œ ì œì™¸ ëª©ë¡ì— ì—†ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ë„ ìˆëŠ” ì»¬ëŸ¼ë“¤\n",
    "        feature_cols = [col for col in numeric_cols \n",
    "                       if col not in exclude_cols and col in self.test_data.columns]\n",
    "        \n",
    "        print(f\"ì „ì²´ ìˆ«ìí˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ\")\n",
    "        print(f\"ì œì™¸ëœ ì»¬ëŸ¼: {len([col for col in numeric_cols if col in exclude_cols])}ê°œ\")\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ê°€ ë§ì€ íŠ¹ì„± ì œê±°\n",
    "        missing_threshold = 0.5\n",
    "        features_to_remove = []\n",
    "        for col in feature_cols.copy():\n",
    "            train_missing = self.train_data[col].isnull().mean()\n",
    "            test_missing = self.test_data[col].isnull().mean()\n",
    "            if train_missing > missing_threshold or test_missing > missing_threshold:\n",
    "                features_to_remove.append(col)\n",
    "                feature_cols.remove(col)\n",
    "                print(f\"ì œê±°: {col} (í›ˆë ¨ ê²°ì¸¡ì¹˜ {train_missing:.1%}, í…ŒìŠ¤íŠ¸ ê²°ì¸¡ì¹˜ {test_missing:.1%})\")\n",
    "        \n",
    "        # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ì•ˆì „ì„± ê²€ì¦\n",
    "        print(f\"\\në°ì´í„° íƒ€ì… ê²€ì¦:\")\n",
    "        for col in feature_cols[:5]:  # ì²˜ìŒ 5ê°œë§Œ í™•ì¸\n",
    "            train_dtype = self.train_data[col].dtype\n",
    "            test_dtype = self.test_data[col].dtype\n",
    "            print(f\"  {col}: í›ˆë ¨={train_dtype}, í…ŒìŠ¤íŠ¸={test_dtype}\")\n",
    "        \n",
    "        # ìµœì¢… íŠ¹ì„± ë°ì´í„° ìƒì„±\n",
    "        X_train = self.train_data[feature_cols].copy()\n",
    "        y_train = self.train_data['congestion'].copy()\n",
    "        X_test = self.test_data[feature_cols].copy()\n",
    "        y_test = self.test_data['congestion'].copy()\n",
    "        \n",
    "        # ë¬¸ìì—´ì´ ì„ì—¬ìˆëŠ”ì§€ ìµœì¢… í™•ì¸\n",
    "        for col in feature_cols:\n",
    "            if X_train[col].dtype == 'object':\n",
    "                print(f\"âš ï¸ ê²½ê³ : {col}ì´ ë¬¸ìì—´ íƒ€ì…ì…ë‹ˆë‹¤. ìƒ˜í”Œ: {X_train[col].head().tolist()}\")\n",
    "                # ë¬¸ìì—´ ì»¬ëŸ¼ì´ë©´ ì œê±°\n",
    "                feature_cols.remove(col)\n",
    "                X_train = X_train.drop(columns=[col])\n",
    "                X_test = X_test.drop(columns=[col])\n",
    "        \n",
    "        # LightGBM í˜¸í™˜ì„±ì„ ìœ„í•œ ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        print(f\"\\nğŸ”§ LightGBM í˜¸í™˜ì„±ì„ ìœ„í•œ ì»¬ëŸ¼ëª… ì •ë¦¬ ì¤‘...\")\n",
    "        def clean_feature_name(name):\n",
    "            \"\"\"íŠ¹ìˆ˜ë¬¸ìë¥¼ ì•ˆì „í•œ ë¬¸ìë¡œ ì¹˜í™˜\"\"\"\n",
    "            # íŠ¹ìˆ˜ë¬¸ìë“¤ì„ ì•ˆì „í•œ ë¬¸ìë¡œ ì¹˜í™˜\n",
    "            replacements = {\n",
    "                '%': 'pct',\n",
    "                '(': '_',\n",
    "                ')': '_',\n",
    "                '[': '_',\n",
    "                ']': '_',\n",
    "                ':': '_',\n",
    "                ' ': '_',\n",
    "                '-': '_',\n",
    "                '/': '_',\n",
    "                '.': '_',\n",
    "                ',': '_',\n",
    "                '&': 'and',\n",
    "                '+': 'plus',\n",
    "                '*': 'mult',\n",
    "                '=': 'eq',\n",
    "                '<': 'lt',\n",
    "                '>': 'gt',\n",
    "                '!': 'not',\n",
    "                '?': 'q',\n",
    "                '@': 'at',\n",
    "                '#': 'hash',\n",
    "                '$': 'dollar'\n",
    "            }\n",
    "            \n",
    "            cleaned_name = name\n",
    "            for old_char, new_char in replacements.items():\n",
    "                cleaned_name = cleaned_name.replace(old_char, new_char)\n",
    "            \n",
    "            # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì •ë¦¬\n",
    "            while '__' in cleaned_name:\n",
    "                cleaned_name = cleaned_name.replace('__', '_')\n",
    "            \n",
    "            # ì‹œì‘ê³¼ ëì˜ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°\n",
    "            cleaned_name = cleaned_name.strip('_')\n",
    "            \n",
    "            return cleaned_name\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬ ë° ë³€ê²½ì‚¬í•­ ì¶”ì \n",
    "        original_feature_cols = feature_cols.copy()\n",
    "        cleaned_feature_cols = [clean_feature_name(col) for col in feature_cols]\n",
    "        \n",
    "        # ë³€ê²½ëœ ì»¬ëŸ¼ëª…ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        changes_made = False\n",
    "        for original, cleaned in zip(original_feature_cols, cleaned_feature_cols):\n",
    "            if original != cleaned:\n",
    "                if not changes_made:\n",
    "                    print(\"  ì»¬ëŸ¼ëª… ë³€ê²½ ì‚¬í•­:\")\n",
    "                    changes_made = True\n",
    "                print(f\"    {original} â†’ {cleaned}\")\n",
    "        \n",
    "        if not changes_made:\n",
    "            print(\"  âœ… ëª¨ë“  ì»¬ëŸ¼ëª…ì´ ì´ë¯¸ ì•ˆì „í•¨\")\n",
    "        \n",
    "        # DataFrame ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "        column_mapping = dict(zip(original_feature_cols, cleaned_feature_cols))\n",
    "        X_train = X_train.rename(columns=column_mapping)\n",
    "        X_test = X_test.rename(columns=column_mapping)\n",
    "        feature_cols = cleaned_feature_cols\n",
    "        \n",
    "        print(f\"\\nìµœì¢… íŠ¹ì„± ìˆ˜: {len(feature_cols)}ê°œ\")\n",
    "        print(f\"íŠ¹ì„± ì¢…ë¥˜: ê¸°ë³¸ì‹œê°„, ê·¹í•œê¸°ìƒ, ìƒí˜¸ì‘ìš©, ê¸°ìƒì‹œì°¨, ì¸ì½”ë”©\")\n",
    "        print(f\"í›ˆë ¨ ë°ì´í„° í˜•íƒœ: {X_train.shape}\")\n",
    "        print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ: {X_test.shape}\")\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test, feature_cols\n",
    "    \n",
    "    def hyperparameter_tuning(self, X_train, y_train, model_type='xgboost'):\n",
    "        \"\"\"Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\"\"\"\n",
    "        print(f\"\\nğŸ” {model_type} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            if model_type == 'xgboost':\n",
    "                params = {\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "                \n",
    "            elif model_type == 'lightgbm':\n",
    "                params = {\n",
    "                    'objective': 'regression',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                    'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': -1\n",
    "                }\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "            \n",
    "            # ì‹œê³„ì—´ êµì°¨ê²€ì¦\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=tscv, \n",
    "                                      scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "            return cv_scores.mean()\n",
    "        \n",
    "        # Optuna ìŠ¤í„°ë””\n",
    "        study = optuna.create_study(direction='maximize', \n",
    "                                  sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {study.best_params}\")\n",
    "        print(f\"ìµœì  CV ì ìˆ˜: {study.best_value:.4f}\")\n",
    "        \n",
    "        return study.best_params\n",
    "    \n",
    "    def feature_selection(self, X_train, y_train, X_test, feature_cols, method='simple'):\n",
    "        \"\"\"ë¹ ë¥¸ íŠ¹ì„± ì„ íƒ\"\"\"\n",
    "        print(f\"\\nğŸ¯ ë¹ ë¥¸ íŠ¹ì„± ì„ íƒ ({method}) ì¤‘...\")\n",
    "        \n",
    "        if method == 'simple' or len(feature_cols) < 20:\n",
    "            # ê°„ë‹¨í•œ ë°©ë²•: ë¶„ì‚°ì´ ë„ˆë¬´ ë‚®ì€ íŠ¹ì„±ë§Œ ì œê±°\n",
    "            from sklearn.feature_selection import VarianceThreshold\n",
    "            \n",
    "            # ë¶„ì‚° ì„ê³„ê°’ìœ¼ë¡œ íŠ¹ì„± ì„ íƒ (ë§¤ìš° ë¹ ë¦„)\n",
    "            selector = VarianceThreshold(threshold=0.01)\n",
    "            X_train_transformed = selector.fit_transform(X_train)\n",
    "            X_test_transformed = selector.transform(X_test)\n",
    "            \n",
    "            selected_features = [feature_cols[i] for i, selected in enumerate(selector.get_support()) if selected]\n",
    "            \n",
    "            print(f\"ë¶„ì‚° ê¸°ë°˜ ì„ íƒ: {len(selected_features)}ê°œ (ì „ì²´ {len(feature_cols)}ê°œ ì¤‘)\")\n",
    "            \n",
    "            X_train_selected = pd.DataFrame(X_train_transformed, columns=selected_features, index=X_train.index)\n",
    "            X_test_selected = pd.DataFrame(X_test_transformed, columns=selected_features, index=X_test.index)\n",
    "            \n",
    "        else:\n",
    "            # ê¸°ì¡´ ì¤‘ìš”ë„ ê¸°ë°˜ ë°©ë²• (ë” ëŠë¦¼)\n",
    "            rf = RandomForestRegressor(n_estimators=20, random_state=42, n_jobs=1)  # ë” ë¹ ë¥´ê²Œ\n",
    "            rf.fit(X_train, y_train)\n",
    "            \n",
    "            importances = rf.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': feature_cols,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            threshold = 0.005  # ì„ê³„ê°’ ì™„í™”\n",
    "            selected_features = feature_importance_df[\n",
    "                feature_importance_df['importance'] >= threshold\n",
    "            ]['feature'].tolist()\n",
    "            \n",
    "            print(f\"ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ: {len(selected_features)}ê°œ\")\n",
    "            \n",
    "            X_train_selected = X_train[selected_features]\n",
    "            X_test_selected = X_test[selected_features]\n",
    "        \n",
    "        self.selected_features = selected_features\n",
    "        return X_train_selected, X_test_selected, selected_features\n",
    "    \n",
    "    def train_enhanced_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"ê³ ë„í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨ (XGBoost, LightGBMë§Œ)\"\"\"\n",
    "        print(\"\\nğŸš€ ê³ ë„í™”ëœ ëª¨ë¸ í›ˆë ¨ ì‹œì‘\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        models_to_train = {\n",
    "            'Enhanced_XGBoost': 'xgboost',\n",
    "            'Enhanced_LightGBM': 'lightgbm'\n",
    "        }\n",
    "        \n",
    "        for model_name, model_type in models_to_train.items():\n",
    "            print(f\"\\nğŸ”§ {model_name} í›ˆë ¨ ì¤‘...\")\n",
    "            \n",
    "            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "            if self.use_optuna:\n",
    "                best_params = self.hyperparameter_tuning(X_train, y_train, model_type)\n",
    "            else:\n",
    "                # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "                if model_type == 'xgboost':\n",
    "                    best_params = {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1}\n",
    "                elif model_type == 'lightgbm':\n",
    "                    best_params = {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1}\n",
    "            \n",
    "            # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\n",
    "            if model_type == 'xgboost':\n",
    "                model = xgb.XGBRegressor(**best_params)\n",
    "            elif model_type == 'lightgbm':\n",
    "                model = lgb.LGBMRegressor(**best_params)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            self.models[model_name] = model\n",
    "            self.results[model_name] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'params': best_params\n",
    "            }\n",
    "            \n",
    "            print(f\"  MAE: {mae:.3f}\")\n",
    "            print(f\"  RMSE: {rmse:.3f}\")\n",
    "            print(f\"  RÂ²: {r2:.3f}\")\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "        best_model_name = min(self.results.keys(), key=lambda x: self.results[x]['mae'])\n",
    "        self.best_model = self.models[best_model_name]\n",
    "        \n",
    "        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\n",
    "        print(f\"  MAE: {self.results[best_model_name]['mae']:.3f}\")\n",
    "        print(f\"  RÂ²: {self.results[best_model_name]['r2']:.3f}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def save_model(self, filename='enhanced_subway_model.pkl'):\n",
    "        \"\"\"ëª¨ë¸ ì €ì¥\"\"\"\n",
    "        model_data = {\n",
    "            'best_model': self.best_model,\n",
    "            'feature_engineer': self.feature_engineer,\n",
    "            'selected_features': self.selected_features,\n",
    "            'results': self.results,\n",
    "            'feature_importance': self.feature_importance\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filename)\n",
    "        print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "\n",
    "    def separate_time_effects(self, method='residual'):\n",
    "        \"\"\"ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬\"\"\"\n",
    "        print(f\"\\nğŸ• ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬ ì‹œì‘ ({method} ë°©ë²•)\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if method == 'residual':\n",
    "            return self._residual_based_separation()\n",
    "        elif method == 'decompose':\n",
    "            return self._seasonal_decompose_separation()\n",
    "        else:\n",
    "            print(\"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°©ë²•ì…ë‹ˆë‹¤. 'residual' ë˜ëŠ” 'decompose'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "            return None\n",
    "    \n",
    "    def _residual_based_separation(self):\n",
    "        \"\"\"ì”ì°¨ ê¸°ë°˜ ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬\"\"\"\n",
    "        print(\"ğŸ“Š ì”ì°¨ ê¸°ë°˜ ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì‹œê°„ ë³€ìˆ˜ë§Œìœ¼ë¡œ ê°„ë‹¨ ëª¨ë¸ í•™ìŠµ\n",
    "        time_features = ['hour', 'dayofweek', 'month', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos']\n",
    "        available_time_features = [f for f in time_features if f in self.train_data.columns]\n",
    "        \n",
    "        print(f\"ì‹œê°„ íŠ¹ì„±: {available_time_features}\")\n",
    "        \n",
    "        # ì‹œê°„ ëª¨ë¸ í•™ìŠµ\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        time_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        \n",
    "        X_time_train = self.train_data[available_time_features]\n",
    "        y_time_train = self.train_data['congestion']\n",
    "        X_time_test = self.test_data[available_time_features]\n",
    "        y_time_test = self.test_data['congestion']\n",
    "        \n",
    "        time_model.fit(X_time_train, y_time_train)\n",
    "        \n",
    "        # ì‹œê°„ íš¨ê³¼ ì˜ˆì¸¡\n",
    "        time_pred_train = time_model.predict(X_time_train)\n",
    "        time_pred_test = time_model.predict(X_time_test)\n",
    "        \n",
    "        # ì”ì°¨ ê³„ì‚° (ì‹œê°„ íš¨ê³¼ ì œê±°)\n",
    "        self.train_data['congestion_residual'] = y_time_train - time_pred_train\n",
    "        self.test_data['congestion_residual'] = y_time_test - time_pred_test\n",
    "        \n",
    "        print(f\"ì‹œê°„ ëª¨ë¸ RÂ²: {time_model.score(X_time_train, y_time_train):.3f}\")\n",
    "        print(f\"ì›ë³¸ í˜¼ì¡ë„ ë¶„ì‚°: {y_time_train.var():.3f}\")\n",
    "        print(f\"ì”ì°¨ ë¶„ì‚°: {self.train_data['congestion_residual'].var():.3f}\")\n",
    "        print(f\"ì‹œê°„ íš¨ê³¼ ì œê±°ìœ¨: {(1 - self.train_data['congestion_residual'].var()/y_time_train.var())*100:.1f}%\")\n",
    "        \n",
    "        return time_model\n",
    "    \n",
    "    def _seasonal_decompose_separation(self):\n",
    "        \"\"\"ì‹œê³„ì—´ ë¶„í•´ ê¸°ë°˜ ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬\"\"\"\n",
    "        print(\"ğŸ“ˆ ì‹œê³„ì—´ ë¶„í•´ ê¸°ë°˜ ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        try:\n",
    "            from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "            \n",
    "            # ì—­ë³„ë¡œ ì‹œê³„ì—´ ë¶„í•´\n",
    "            for data_name, data in [('train', self.train_data), ('test', self.test_data)]:\n",
    "                print(f\"{data_name} ë°ì´í„° ë¶„í•´ ì¤‘...\")\n",
    "                \n",
    "                residuals = []\n",
    "                for station in data['station_name'].unique():\n",
    "                    station_data = data[data['station_name'] == station].sort_values('tm')\n",
    "                    \n",
    "                    if len(station_data) >= 48:  # ìµœì†Œ 2ì¼ ë°ì´í„°\n",
    "                        try:\n",
    "                            # ì‹œê³„ì—´ ë¶„í•´ (24ì‹œê°„ ì£¼ê¸°)\n",
    "                            decomposition = seasonal_decompose(\n",
    "                                station_data['congestion'], \n",
    "                                model='additive', \n",
    "                                period=24,\n",
    "                                extrapolate_trend='freq'\n",
    "                            )\n",
    "                            \n",
    "                            station_residuals = decomposition.resid.fillna(0)\n",
    "                            residuals.extend(zip(station_data.index, station_residuals))\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"âš ï¸ {station} ì—­ ë¶„í•´ ì‹¤íŒ¨: {str(e)}\")\n",
    "                            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ê°’ ì‚¬ìš©\n",
    "                            residuals.extend(zip(station_data.index, station_data['congestion']))\n",
    "                    else:\n",
    "                        # ë°ì´í„° ë¶€ì¡± ì‹œ ì›ë³¸ê°’ ì‚¬ìš©  \n",
    "                        residuals.extend(zip(station_data.index, station_data['congestion']))\n",
    "                \n",
    "                # ì”ì°¨ ì„¤ì •\n",
    "                residual_dict = dict(residuals)\n",
    "                data['congestion_residual'] = data.index.map(residual_dict).fillna(data['congestion'])\n",
    "                \n",
    "                print(f\"{data_name} ì›ë³¸ ë¶„ì‚°: {data['congestion'].var():.3f}\")\n",
    "                print(f\"{data_name} ì”ì°¨ ë¶„ì‚°: {data['congestion_residual'].var():.3f}\")\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ statsmodels ë¯¸ì„¤ì¹˜. pip install statsmodels\")\n",
    "            return self._residual_based_separation()  # ëŒ€ì•ˆ ì‚¬ìš©\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬ í¬í•¨\"\"\"\n",
    "    print(\"ğŸš€ ê³ ë„í™”ëœ ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ ëª¨ë¸ (ì‹œê°„íš¨ê³¼ë¶„ë¦¬)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = EnhancedSubwayModel(use_optuna=True, n_trials=10)\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "    if not model.load_and_preprocess_data(train_years=['21'], test_year='23', sample_size=5000000):\n",
    "        return None\n",
    "    \n",
    "    # 2. ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬ (ì˜µì…˜)\n",
    "    use_time_separation = True  # ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€\n",
    "    separation_method = 'residual'  # 'residual' ë˜ëŠ” 'decompose'\n",
    "    \n",
    "    if use_time_separation:\n",
    "        model.separate_time_effects(method=separation_method)\n",
    "        target_col = 'congestion_residual'  # ì”ì°¨ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©\n",
    "        print(f\"âœ… íƒ€ê²Ÿ ë³€ìˆ˜: {target_col} (ì‹œê°„ íš¨ê³¼ ì œê±°ë¨)\")\n",
    "    else:\n",
    "        target_col = 'congestion'  # ì›ë³¸ ì‚¬ìš©\n",
    "        print(f\"âœ… íƒ€ê²Ÿ ë³€ìˆ˜: {target_col} (ì›ë³¸)\")\n",
    "    \n",
    "    # 3. íŠ¹ì„± ì¤€ë¹„ (ì‹œê°„ íš¨ê³¼ ë¶„ë¦¬ëœ íƒ€ê²Ÿ ì‚¬ìš©)\n",
    "    original_congestion = model.train_data['congestion'].copy()\n",
    "    model.train_data['congestion'] = model.train_data[target_col]\n",
    "    model.test_data['congestion'] = model.test_data[target_col]\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, feature_cols = model.prepare_features()\n",
    "    \n",
    "    # 4. íŠ¹ì„± ì„ íƒ\n",
    "    X_train_selected, X_test_selected, selected_features = model.feature_selection(\n",
    "        X_train, y_train, X_test, feature_cols, method='simple'\n",
    "    )\n",
    "    \n",
    "    # 5. ëª¨ë¸ í›ˆë ¨\n",
    "    results = model.train_enhanced_models(X_train_selected, y_train, X_test_selected, y_test)\n",
    "    \n",
    "    # 6. ëª¨ë¸ ì €ì¥\n",
    "    try:\n",
    "        filename = f'enhanced_model_{\"time_separated\" if use_time_separation else \"original\"}.pkl'\n",
    "        model.save_model(filename)\n",
    "    except:\n",
    "        model.save_model('enhanced_model_time_separated.pkl')\n",
    "    \n",
    "    print(f\"\\nğŸ‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    return model, X_test_selected, y_test, selected_features, results, use_time_separation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "model, X_test, y_test, selected_features, results, use_time_separation = train_model()\n",
    "\n",
    "# ë¶„ì„ìš© ë°ì´í„° ì €ì¥\n",
    "import pickle\n",
    "analysis_data = {\n",
    "    'model': model,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test, \n",
    "    'selected_features': selected_features,\n",
    "    'results': results,\n",
    "    'use_time_separation': use_time_separation,\n",
    "    'train_data': model.train_data,\n",
    "    'test_data': model.test_data\n",
    "}\n",
    "\n",
    "with open('model_data_for_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(analysis_data, f)\n",
    "\n",
    "print(\"âœ… ë¶„ì„ìš© ë°ì´í„° ì €ì¥ ì™„ë£Œ: model_data_for_analysis.pkl\")\n",
    "print(f\"ì €ì¥ëœ ëª¨ë¸ ìˆ˜: {len(results)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(y_test):,}\")\n",
    "print(f\"ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(selected_features)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
