{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ë° í•œê¸€ í°íŠ¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # DB ë¶„ì„ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ -> DataFrame, Series ë“± DB êµ¬ì¡° ì œê³µ\n",
    "import numpy as np                  # ìˆ˜ì¹˜ ì—°ì‚° ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ -> ìˆ˜ì¹˜ ê³„ì‚°, ë°°ì—´ ì—°ì‚°\n",
    "import matplotlib.pyplot as plt     # ì‹œê°í™” ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ -> ê·¸ë˜í”„, ì°¨íŠ¸ ê·¸ë¦¬ê¸°\n",
    "import seaborn as sns               # ì‹œê°í™” ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ -> matplolib ê¸°ë°˜ ê³ ê¸‰ í†µê³„ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from scipy import stats             # í†µê³„ ì—°ì‚° ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ -> í†µê³„ ë¶„ì„, í™•ë¥  ë¶„í¬ ê´€ë ¨ í•¨ìˆ˜ë“¤ ì œê³µ\n",
    "import warnings                     # ê²½ê³  ë©”ì„¸ì§€ ì œì–´\n",
    "warnings.filterwarnings('ignore')   # ê²½ê³  ë©”ì„¸ì§€ ë¬´ì‹œ\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "import matplotlib.font_manager as fm\n",
    "font_list = [font.name for font in fm.fontManager.ttflist if 'Gothic' in font.name or 'Malgun' in font.name]\n",
    "if font_list:\n",
    "    plt.rcParams['font.family'] = font_list[0]\n",
    "else:\n",
    "    # ëŒ€ì•ˆ í°íŠ¸ë“¤\n",
    "    try:\n",
    "        plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "    except:\n",
    "        try:\n",
    "            plt.rcParams['font.family'] = 'Microsoft YaHei'\n",
    "        except:\n",
    "            plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False   # ë§ˆì´ë„ˆìŠ¤ í‘œì‹œ ìœ„í•œ ì„¤ì •\n",
    "\n",
    "print(\"=== ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ íƒìƒ‰ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì‹œê°„ë³€ìˆ˜ ì¶”ì¶œ ë“±ë“±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë”© (ìê¸° íŒŒì¼ ìœ„ì¹˜ë¡œ ë°”ê¿”ì„œ í•˜ì„¸ìš”)\n",
    "df = pd.read_csv(r'C:\\Users\\jueun\\Desktop\\Uni\\ê¸°í›„ë¹…ë°ì´í„°\\CLIMAX\\train_subway_2021.csv', encoding='utf-8')\n",
    "\n",
    "print(\"=== ğŸ” ì´ˆê¸° ë°ì´í„° í™•ì¸ ===\")\n",
    "print(f\"ğŸ“Š ë°ì´í„° shape: {df.shape}\")       # dataframeì˜ í¬ê¸°(í–‰, ì—´ ìˆ˜) ì¶œë ¥ -> f-string ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ ê°’ì„ ë¬¸ìì—´ì— ì‚½ì…\n",
    "print(f\"ğŸ“‹ ì»¬ëŸ¼ëª…: {df.columns.tolist()}\")  # dataframeì˜ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶œë ¥\n",
    "\n",
    "# dataframeì˜ ì „ì²´ì ì¸ ì •ë³´ ì¶œë ¥ (ê° ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…, ê²°ì¸¡ì¹˜ ê°œìˆ˜, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±)\n",
    "print(\"\\nğŸ“ˆ ê¸°ë³¸ ì •ë³´:\")\n",
    "df.info()\n",
    "\n",
    "# ë°ì´í„°ì˜ ìƒìœ„ 5í–‰ ì„¹ì…˜ ì œëª© ì¶œë ¥\n",
    "print(\"\\nğŸ“„ ìƒìœ„ 5í–‰:\")\n",
    "print(df.head())\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (-99ë¥¼ NaNìœ¼ë¡œ ë³€í™˜)\n",
    "print(\"\\nğŸš¨ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...\")\n",
    "weather_cols = ['ta', 'ws', 'rn_hr1', 'hm', 'si', 'ta_chi']\n",
    "for col in weather_cols:\n",
    "    before_count = (df[col] == -99).sum()\n",
    "    df[col] = df[col].replace(-99, np.nan)\n",
    "    print(f\"{col}: -99 ê°’ {before_count:,}ê°œë¥¼ NaNìœ¼ë¡œ ë³€í™˜\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë¶„ì„\n",
    "print(\"\\nğŸ“Š ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë¶„ì„:\")\n",
    "total_records = len(df)\n",
    "for col in weather_cols:\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    missing_rate = (missing_count / total_records) * 100\n",
    "    print(f\"{col}: {missing_count:,}ê°œ ({missing_rate:.2f}%)\")\n",
    "\n",
    "print(f\"\\nì „ì²´ ë°ì´í„°: {total_records:,}ê°œ\")\n",
    "print(f\"ëª¨ë“  ê¸°í›„ë³€ìˆ˜ê°€ ì™„ì „í•œ í–‰: {len(df[weather_cols].dropna()):,}ê°œ ({len(df[weather_cols].dropna())/total_records*100:.2f}%)\")\n",
    "\n",
    "# ì‹œê°„ ë³€ìˆ˜ ì¶”ì¶œ\n",
    "## í˜„ì¬ tm ì»¬ëŸ¼ì€ ë…„ì›”ì¼ì‹œê°„ì´ í•©ì³ì§„ ìƒíƒœì„. -> ì‹œê°„ëŒ€ë³„ ë¶„ì„ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì‹œê°„ ë³€ìˆ˜ ì¶”ì¶œ\n",
    "df['m_hr1'] = df['tm'] % 100\n",
    "\n",
    "print(f\"\\nâ° ì‹œê°„ ë³€ìˆ˜ ì¶”ì¶œ í™•ì¸:\")\n",
    "print(f\"tm ìƒ˜í”Œ: {df['tm'].head().tolist()}\")\n",
    "print(f\"ì¶”ì¶œëœ ì‹œê°„ ë²”ìœ„: {df['m_hr1'].min()} ~ {df['m_hr1'].max()}\")\n",
    "print(df['m_hr1'].value_counts().sort_index())\n",
    "print(f\"ì‹œê°„ ë¶„í¬ (ìƒìœ„ 10ê°œ):\")\n",
    "print(df['m_hr1'].value_counts().sort_index().head(10))\n",
    "\n",
    "# ì¼ì‚¬ëŸ‰ ì•¼ê°„ì‹œê°„ëŒ€ ë¬¼ë¦¬ì  ì²˜ë¦¬\n",
    "print(\"\\nğŸŒ™ ì¼ì‚¬ëŸ‰ ì•¼ê°„ì‹œê°„ëŒ€ ì²˜ë¦¬ ì¤‘...\")\n",
    "night_hours = [0, 1, 2, 3, 4, 5, 6, 19, 20, 21, 22, 23]\n",
    "night_mask = df['m_hr1'].isin(night_hours)\n",
    "night_si_missing = df.loc[night_mask, 'si'].isnull().sum()\n",
    "df.loc[night_mask, 'si'] = 0.0\n",
    "print(f\"ì•¼ê°„ì‹œê°„ëŒ€({night_hours}) ì¼ì‚¬ëŸ‰ ê²°ì¸¡ê°’ {night_si_missing:,}ê°œë¥¼ 0ìœ¼ë¡œ ì„¤ì •\")\n",
    "\n",
    "# ë™ì‹œ ê²°ì¸¡ íŒ¨í„´ í™•ì¸\n",
    "print(\"\\nğŸ” ë™ì‹œ ê²°ì¸¡ íŒ¨í„´ í™•ì¸:\")\n",
    "basic_weather = ['ta', 'ws', 'rn_hr1', 'hm']\n",
    "df['missing_count'] = df[basic_weather].isnull().sum(axis=1)\n",
    "df['multiple_missing'] = df['missing_count'] >= 2\n",
    "print(f\"2ê°œ ì´ìƒ ë™ì‹œ ê²°ì¸¡: {df['multiple_missing'].sum():,}ê°œ ({df['multiple_missing'].sum()/len(df)*100:.2f}%)\")\n",
    "\n",
    "# ë³€ìˆ˜ë“¤ ëŒ€í•œ ê¸°ìˆ í†µê³„(í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Ÿê°’, ìµœëŒ“ê°’ ë“±)\n",
    "print(\"\\nğŸ“Š ê¸°ë³¸ í†µê³„:\")\n",
    "print(df[weather_cols + ['congestion', 'm_hr1']].describe())\n",
    "\n",
    "# í˜¼ì¡ë„ ê´€ë ¨ ë²”ìœ„, í‰ê· , ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "print(\"\\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸:\")\n",
    "target_col = 'congestion'\n",
    "print(f\"í˜¼ì¡ë„ ë²”ìœ„: {df[target_col].min():.2f} ~ {df[target_col].max():.2f}\")\n",
    "print(f\"í˜¼ì¡ë„ í‰ê· : {df[target_col].mean():.2f}\")\n",
    "print(f\"í˜¼ì¡ë„ ê²°ì¸¡ì¹˜: {df[target_col].isnull().sum()}ê°œ\")\n",
    "\n",
    "# ë‚ ì”¨ ë³€ìˆ˜ ê´€ë ¨ ë²”ìœ„ ë° ê²°ì¸¡ì¹˜ ìˆ˜\n",
    "print(\"\\nğŸŒ¡ï¸ ë‚ ì”¨ ë³€ìˆ˜ í™•ì¸:\")\n",
    "for col in weather_cols:\n",
    "    non_null_data = df[col].dropna()\n",
    "    if len(non_null_data) > 0:\n",
    "        print(f\"{col}: {non_null_data.min():.2f} ~ {non_null_data.max():.2f} (ê²°ì¸¡ì¹˜: {df[col].isnull().sum():,}ê°œ)\")\n",
    "    else:\n",
    "        print(f\"{col}: ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¹˜\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ë°ì´í„° í˜„í™©:\")\n",
    "print(f\"ì „ì²´ í–‰ ìˆ˜: {len(df):,}\")\n",
    "print(f\"ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ì™„ì „í•œ í–‰ ìˆ˜: {len(df.dropna()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ê¸°ìƒì¡°ê±´ë³„ ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ìƒ ì¡°ê±´ë³„ ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„\n",
    "print(\"ğŸŒ¦ï¸ ê¸°ìƒ ì¡°ê±´ë³„ ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ì‹œê°„ëŒ€ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨\n",
    "print(\"\\nâ° ì‹œê°„ëŒ€ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨:\")\n",
    "missing_by_hour = df.groupby('m_hr1')[weather_cols].apply(lambda x: x.isnull().sum()/len(x)*100)\n",
    "print(missing_by_hour.round(2))\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ê°€ ë§ì€ ì‹œê°„ëŒ€ í™•ì¸\n",
    "print(\"\\nğŸš¨ ê²°ì¸¡ì¹˜ê°€ 30% ì´ìƒì¸ ì‹œê°„ëŒ€-ë³€ìˆ˜ ì¡°í•©:\")\n",
    "for hour in missing_by_hour.index:\n",
    "    for col in weather_cols:\n",
    "        missing_rate = missing_by_hour.loc[hour, col]\n",
    "        if missing_rate >= 30:\n",
    "            print(f\"  {hour}ì‹œ {col}: {missing_rate:.1f}%\")\n",
    "\n",
    "# ê³„ì ˆë³„ ê²°ì¸¡ì¹˜ íŒ¨í„´ (ì›”ë³„ë¡œ í™•ì¸)\n",
    "df['month'] = (df['tm'] // 100) % 100  # ì›” ì¶”ì¶œ\n",
    "print(\"\\nğŸ“… ì›”ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨:\")\n",
    "missing_by_month = df.groupby('month')[weather_cols].apply(lambda x: x.isnull().sum()/len(x)*100)\n",
    "print(missing_by_month.round(2))\n",
    "\n",
    "# ê¸°ìƒ ì¡°ê±´ ê°„ ê²°ì¸¡ì¹˜ ë™ì‹œ ë°œìƒ íŒ¨í„´\n",
    "print(\"\\nğŸ”— ê¸°ìƒë³€ìˆ˜ ê°„ ê²°ì¸¡ì¹˜ ë™ì‹œ ë°œìƒ íŒ¨í„´:\")\n",
    "missing_pattern = df[weather_cols].isnull()\n",
    "for i, col1 in enumerate(weather_cols):\n",
    "    for col2 in weather_cols[i+1:]:\n",
    "        both_missing = (missing_pattern[col1] & missing_pattern[col2]).sum()\n",
    "        total_missing_col1 = missing_pattern[col1].sum()\n",
    "        if total_missing_col1 > 0:\n",
    "            overlap_rate = both_missing / total_missing_col1 * 100\n",
    "            if overlap_rate > 50:  # 50% ì´ìƒ ê²¹ì¹˜ëŠ” ê²½ìš°ë§Œ ì¶œë ¥\n",
    "                print(f\"  {col1}-{col2}: {overlap_rate:.1f}% ({both_missing:,}ê°œ ë™ì‹œ ê²°ì¸¡)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ê²°ì¸¡ì¹˜ íŒ¨í„´ ì‹œê°í™” (íˆíŠ¸ë§µ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ íŒ¨í„´ ì‹œê°í™”\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1) ì‹œê°„ëŒ€ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ íˆíŠ¸ë§µ\n",
    "plt.subplot(2, 2, 1)\n",
    "missing_by_hour_pivot = missing_by_hour.T\n",
    "sns.heatmap(missing_by_hour_pivot, annot=True, cmap='Reds', fmt='.1f', cbar=True)\n",
    "plt.title('ì‹œê°„ëŒ€ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (%)')\n",
    "plt.xlabel('ì‹œê°„')\n",
    "plt.ylabel('ê¸°ìƒë³€ìˆ˜')\n",
    "\n",
    "# 2) ì›”ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨\n",
    "plt.subplot(2, 2, 2)\n",
    "missing_by_month_pivot = missing_by_month.T\n",
    "sns.heatmap(missing_by_month_pivot, annot=True, cmap='Reds', fmt='.1f', cbar=True)\n",
    "plt.title('ì›”ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (%)')\n",
    "plt.xlabel('ì›”')\n",
    "plt.ylabel('ê¸°ìƒë³€ìˆ˜')\n",
    "\n",
    "# 3) ê²°ì¸¡ì¹˜ íŒ¨í„´ ìƒê´€ê´€ê³„\n",
    "plt.subplot(2, 2, 3)\n",
    "missing_corr = df[weather_cols].isnull().corr()\n",
    "sns.heatmap(missing_corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('ê¸°ìƒë³€ìˆ˜ ê°„ ê²°ì¸¡ì¹˜ íŒ¨í„´ ìƒê´€ê´€ê³„')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„°ì…‹ êµ¬ì„± (3ê°€ì§€) - í¬ê²Œ ì°¨ì´ ì—†ì—ˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "print(\"ğŸ“Š ë°ì´í„°ì…‹ êµ¬ì„± ì „ëµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dataset A: ì™„ì „í•œ ë°ì´í„° (ê³ ì‹ ë¢°ì„±)\n",
    "df_complete = df.dropna(subset=['ta', 'ws', 'rn_hr1', 'hm', 'si', 'ta_chi']).copy()\n",
    "print(f\"\\nğŸ”’ Dataset A (ì™„ì „í•œ ë°ì´í„°): {len(df_complete):,}ê°œ ({len(df_complete)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Dataset B: ê¸°ë³¸ ê¸°í›„ë³€ìˆ˜ë§Œ ì™„ì „í•œ ë°ì´í„°\n",
    "df_basic = df.dropna(subset=['ta', 'ws', 'rn_hr1', 'hm']).copy()\n",
    "print(f\"ğŸ“‹ Dataset B (ê¸°ë³¸ ê¸°í›„ë³€ìˆ˜): {len(df_basic):,}ê°œ ({len(df_basic)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Dataset C: ë³´ê°„ëœ ë°ì´í„°\n",
    "df_interpolated = df.copy()\n",
    "\n",
    "# ê¸°ë³¸ ê¸°í›„ë³€ìˆ˜ ë³´ê°„ (ë™ì‹œ ê²°ì¸¡ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)\n",
    "print(f\"\\nğŸ”§ ë³´ê°„ ì²˜ë¦¬ ì¤‘...\")\n",
    "for col in ['ta', 'ws', 'hm']:\n",
    "    # ë™ì‹œ ê²°ì¸¡ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ë³´ê°„\n",
    "    mask = ~df_interpolated['multiple_missing']\n",
    "    before_missing = df_interpolated[col].isnull().sum()\n",
    "    df_interpolated.loc[mask, f'{col}_interpolated'] = df_interpolated.loc[mask, col].interpolate(method='linear')\n",
    "    # ë™ì‹œ ê²°ì¸¡ì¸ ê²½ìš°ëŠ” ì›ë˜ ê°’ ìœ ì§€\n",
    "    df_interpolated.loc[~mask, f'{col}_interpolated'] = df_interpolated.loc[~mask, col]\n",
    "    after_missing = df_interpolated[f'{col}_interpolated'].isnull().sum()\n",
    "    print(f\"  {col}: {before_missing:,}ê°œ â†’ {after_missing:,}ê°œ ê²°ì¸¡ ({before_missing-after_missing:,}ê°œ ë³´ê°„)\")\n",
    "\n",
    "# ê°•ìˆ˜ëŸ‰ì€ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ë¹„ê°€ ì•ˆ ì™”ì„ ê°€ëŠ¥ì„±)\n",
    "rn_before = df_interpolated['rn_hr1'].isnull().sum()\n",
    "df_interpolated['rn_hr1_filled'] = df_interpolated['rn_hr1'].fillna(0)\n",
    "print(f\"  rn_hr1: {rn_before:,}ê°œ ê²°ì¸¡ì„ 0ìœ¼ë¡œ ì±„ì›€\")\n",
    "\n",
    "# Dataset C ì™„ì„±\n",
    "interpolated_cols = ['ta_interpolated', 'ws_interpolated', 'rn_hr1_filled', 'hm_interpolated', 'si']\n",
    "df_filled = df_interpolated.dropna(subset=interpolated_cols).copy()\n",
    "print(f\"ğŸ“ˆ Dataset C (ë³´ê°„ í¬í•¨): {len(df_filled):,}ê°œ ({len(df_filled)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° í™œìš© ì „ëµ:\")\n",
    "print(f\"  - í•µì‹¬ ë¶„ì„: Dataset A ì‚¬ìš© (ì‹ ë¢°ì„± ìµœìš°ì„ )\")\n",
    "print(f\"  - íŒ¨í„´ íƒìƒ‰: Dataset B ì‚¬ìš© (ì¼ì‚¬ëŸ‰ ì œì™¸)\")\n",
    "print(f\"  - ë³´ì¡° ê²€ì¦: Dataset C ì‚¬ìš© (ëŒ€ìš©ëŸ‰ ë¶„ì„)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ ê¸°ë³¸ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ ë¶„ì„\n",
    "print(\"\\nğŸ• ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ ë¶„ì„:\")\n",
    "hourly_congestion = df.groupby('m_hr1')['congestion'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "print(hourly_congestion.round(2))\n",
    "\n",
    "# ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ í‰ê·  ì •ë ¬\n",
    "print(\"\\nğŸ“ˆ í˜¼ì¡ë„ê°€ ë†’ì€ ì‹œê°„ëŒ€ TOP 10:\")\n",
    "hourly_mean = df.groupby('m_hr1')['congestion'].mean().sort_values(ascending=False)\n",
    "print(hourly_mean.head(10).round(2))\n",
    "\n",
    "print(\"\\nğŸ“‰ í˜¼ì¡ë„ê°€ ë‚®ì€ ì‹œê°„ëŒ€ TOP 10:\")\n",
    "print(hourly_mean.tail(10).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ ì‹œê°í™”\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 0~23ì‹œê°„ ì „ì²´ ë²”ìœ„\n",
    "all_hours = range(0, 24)\n",
    "\n",
    "# 1) ì‹œê°„ëŒ€ë³„ í‰ê·  í˜¼ì¡ë„ ë¼ì¸ ê·¸ë˜í”„\n",
    "plt.subplot(2, 2, 1)\n",
    "hourly_mean_complete = df.groupby('m_hr1')['congestion'].mean().reindex(all_hours, fill_value=0)\n",
    "plt.plot(all_hours, hourly_mean_complete.values, marker='o', linewidth=2)\n",
    "plt.title('ì‹œê°„ëŒ€ë³„ í‰ê·  í˜¼ì¡ë„')\n",
    "plt.xlabel('ì‹œê°„')\n",
    "plt.ylabel('í‰ê·  í˜¼ì¡ë„')\n",
    "plt.xlim(-0.5, 23.5)\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2) ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ ë°•ìŠ¤í”Œë¡¯\n",
    "plt.subplot(2, 2, 2)\n",
    "df.boxplot(column='congestion', by='m_hr1', ax=plt.gca())\n",
    "plt.title('ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ ë¶„í¬')\n",
    "plt.xlabel('ì‹œê°„')\n",
    "plt.ylabel('í˜¼ì¡ë„')\n",
    "\n",
    "# 3) ì‹œê°„ëŒ€ë³„ ë°ì´í„° ê°œìˆ˜\n",
    "plt.subplot(2, 2, 3)\n",
    "hourly_count_complete = df['m_hr1'].value_counts().reindex(all_hours, fill_value=0)\n",
    "plt.bar(all_hours, hourly_count_complete.values)\n",
    "plt.title('ì‹œê°„ëŒ€ë³„ ë°ì´í„° ê°œìˆ˜')\n",
    "plt.xlabel('ì‹œê°„')\n",
    "plt.ylabel('ë°ì´í„° ê°œìˆ˜')\n",
    "plt.xlim(-0.5, 23.5)\n",
    "plt.xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ìƒˆë²½ì‹œê°„ëŒ€(1-4ì‹œ) ë°ì´í„° ìƒì„¸ í™•ì¸ (ê²°ì¸¡ê°’ì¸ì§€ ë°ì´í„°ê°€ ì—†ëŠ”ì§€ 0ì¸ì§€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-4ì‹œ ë°ì´í„° ìƒì„¸ í™•ì¸\n",
    "for hour in [1, 2, 3, 4]:\n",
    "    hour_data = df[df['m_hr1'] == hour]['congestion']\n",
    "    print(f\"\\n{hour}ì‹œ í˜¼ì¡ë„ ë¶„ì„:\")\n",
    "    print(f\"ë°ì´í„° ê°œìˆ˜: {len(hour_data)}\")\n",
    "    print(f\"0ì¸ ë°ì´í„° ê°œìˆ˜: {(hour_data == 0).sum()}\")\n",
    "    print(f\"0ì´ ì•„ë‹Œ ë°ì´í„° ê°œìˆ˜: {(hour_data != 0).sum()}\")\n",
    "    print(f\"ì‹¤ì œ ê°’ë“¤ (ì²˜ìŒ 10ê°œ): {hour_data.head(10).tolist()}\")\n",
    "    print(f\"ê³ ìœ ê°’ ê°œìˆ˜: {hour_data.nunique()}\")\n",
    "    print(f\"ê³ ìœ ê°’ë“¤: {sorted(hour_data.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. ì‹œê°„ëŒ€ ê·¸ë£¹ ë¶„ë¥˜ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1~4ì‹œ ì œì™¸, 5~0ì‹œ ê·¸ë£¹í•‘\n",
    "def categorize_time_group(hour):\n",
    "    if hour in [1, 2, 3, 4]:\n",
    "        return 'ìš´í–‰ì¤‘ë‹¨'  # ë¶„ì„ì—ì„œ ì œì™¸\n",
    "    elif hour in [5, 6]:\n",
    "        return 'ìƒˆë²½_ì²«ì°¨(5-6ì‹œ)'\n",
    "    elif hour in [7, 8, 9]:\n",
    "        return 'ì•„ì¹¨_ëŸ¬ì‹œì•„ì›Œ(7-9ì‹œ)'\n",
    "    elif hour in [10, 11]:\n",
    "        return 'ì˜¤ì „_ì¼ë°˜(10-11ì‹œ)'\n",
    "    elif hour in [12, 13, 14]:\n",
    "        return 'ì ì‹¬ì‹œê°„(12-14ì‹œ)'\n",
    "    elif hour in [15, 16]:\n",
    "        return 'ì˜¤í›„_ì¼ë°˜(15-16ì‹œ)'\n",
    "    elif hour in [17, 18, 19]:\n",
    "        return 'ì €ë…_ëŸ¬ì‹œì•„ì›Œ(17-19ì‹œ)'\n",
    "    elif hour in [20, 21, 22]:\n",
    "        return 'ì•¼ê°„_ì¼ë°˜(20-22ì‹œ)'\n",
    "    elif hour == 23:\n",
    "        return 'ë§‰ì°¨ì‹œê°„(23ì‹œ)'\n",
    "    elif hour == 0:\n",
    "        return 'ì‹¬ì•¼_ê·€ê°€(0ì‹œ)'\n",
    "    \n",
    "df['time_group'] = df['m_hr1'].apply(categorize_time_group)\n",
    "\n",
    "# ìš´í–‰ì¤‘ë‹¨ ì‹œê°„ ì œì™¸\n",
    "df_valid = df[df['time_group'] != 'ìš´í–‰ì¤‘ë‹¨'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. ì‹œê°„ ê·¸ë£¹ë³„ í˜¼ì¡ë„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ ê·¸ë£¹ë³„ í˜¼ì¡ë„ ë¶„ì„\n",
    "print(\"ğŸ• ì‹œê°„ ê·¸ë£¹ë³„ í˜¼ì¡ë„ ë¶„ì„:\")\n",
    "group_analysis = df_valid.groupby('time_group')['congestion'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "\n",
    "# ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ì„ ìœ„í•œ ìˆœì„œ ì •ì˜\n",
    "time_order = ['ìƒˆë²½_ì²«ì°¨(5-6ì‹œ)', 'ì•„ì¹¨_ëŸ¬ì‹œì•„ì›Œ(7-9ì‹œ)', 'ì˜¤ì „_ì¼ë°˜(10-11ì‹œ)', 'ì ì‹¬ì‹œê°„(12-14ì‹œ)', \n",
    "             'ì˜¤í›„_ì¼ë°˜(15-16ì‹œ)', 'ì €ë…_ëŸ¬ì‹œì•„ì›Œ(17-19ì‹œ)', 'ì•¼ê°„_ì¼ë°˜(20-22ì‹œ)', 'ë§‰ì°¨ì‹œê°„(23ì‹œ)', 'ì‹¬ì•¼_ê·€ê°€(0ì‹œ)']\n",
    "\n",
    "group_analysis_ordered = group_analysis.reindex(time_order)\n",
    "print(group_analysis_ordered.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. ì‹œê°„ ê·¸ë£¹ë³„ í˜¼ì¡ë„ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ ê·¸ë£¹ë³„ ì‹œê°í™”\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# time_groupì„ ìˆœì„œê°€ ìˆëŠ” categoricalë¡œ ë³€í™˜\n",
    "df_valid['time_group_ordered'] = pd.Categorical(df_valid['time_group'], \n",
    "                                              categories=time_order, \n",
    "                                              ordered=True)\n",
    "\n",
    "# 1) ê·¸ë£¹ë³„ í‰ê·  í˜¼ì¡ë„\n",
    "plt.subplot(2, 2, 1)\n",
    "group_means = df_valid.groupby('time_group_ordered')['congestion'].mean()\n",
    "bars = plt.bar(range(len(group_means)), group_means.values, \n",
    "               color=['lightblue', 'red', 'green', 'orange', 'green', 'red', 'purple', 'navy', 'black'])\n",
    "plt.title('ì‹œê°„ ê·¸ë£¹ë³„ í‰ê·  í˜¼ì¡ë„')\n",
    "plt.xlabel('ì‹œê°„ ê·¸ë£¹')\n",
    "plt.ylabel('í‰ê·  í˜¼ì¡ë„')\n",
    "plt.xticks(range(len(group_means)), group_means.index, rotation=45, ha='right')\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for i, v in enumerate(group_means.values):\n",
    "    plt.text(i, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# 2) ê·¸ë£¹ë³„ ë°•ìŠ¤í”Œë¡¯ (ìˆœì„œ ìˆ˜ì •ë¨)\n",
    "plt.subplot(2, 2, 2)\n",
    "df_valid.boxplot(column='congestion', by='time_group_ordered', ax=plt.gca())\n",
    "plt.title('ì‹œê°„ ê·¸ë£¹ë³„ í˜¼ì¡ë„ ë¶„í¬')\n",
    "plt.xlabel('ì‹œê°„ ê·¸ë£¹')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.suptitle('')  # pandas boxplotì˜ ê¸°ë³¸ ì œëª© ì œê±°\n",
    "\n",
    "# 3) ê·¸ë£¹ë³„ ë°ì´í„° ê°œìˆ˜\n",
    "plt.subplot(2, 2, 3)\n",
    "group_counts = df_valid['time_group_ordered'].value_counts().sort_index()  # categorical ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "plt.bar(range(len(group_counts)), group_counts.values, color='lightcoral')\n",
    "plt.title('ì‹œê°„ ê·¸ë£¹ë³„ ë°ì´í„° ê°œìˆ˜')\n",
    "plt.xlabel('ì‹œê°„ ê·¸ë£¹')\n",
    "plt.ylabel('ë°ì´í„° ê°œìˆ˜')\n",
    "plt.xticks(range(len(group_counts)), group_counts.index, rotation=45, ha='right')\n",
    "\n",
    "# 4) ëŸ¬ì‹œì•„ì›Œ vs ë¹„ëŸ¬ì‹œì•„ì›Œ ë¹„êµ\n",
    "plt.subplot(2, 2, 4)\n",
    "rush_comparison = df_valid.groupby(df_valid['time_group'].isin(['ì•„ì¹¨_ëŸ¬ì‹œì•„ì›Œ(7-9ì‹œ)', 'ì €ë…_ëŸ¬ì‹œì•„ì›Œ(17-19ì‹œ)']))['congestion'].mean()\n",
    "rush_labels = ['ì¼ë°˜ì‹œê°„', 'ëŸ¬ì‹œì•„ì›Œ']\n",
    "plt.bar(rush_labels, rush_comparison.values, color=['lightgreen', 'red'])\n",
    "plt.title('ëŸ¬ì‹œì•„ì›Œ vs ì¼ë°˜ì‹œê°„ í‰ê·  í˜¼ì¡ë„')\n",
    "plt.ylabel('í‰ê·  í˜¼ì¡ë„')\n",
    "\n",
    "for i, v in enumerate(rush_comparison.values):\n",
    "    plt.text(i, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. ê¸°í›„ë³€ìˆ˜ ë²”ì£¼í™” í•¨ìˆ˜ë“¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_temperature(temp):\n",
    "    if temp <= 10:\n",
    "        return 'ì €ì˜¨'\n",
    "    elif temp <= 25:\n",
    "        return 'ì¤‘ì˜¨'\n",
    "    else:\n",
    "        return 'ê³ ì˜¨'\n",
    "\n",
    "def categorize_humidity(humidity):\n",
    "    if humidity <= 40:\n",
    "        return 'ì €ìŠµ'\n",
    "    elif humidity <= 60:\n",
    "        return 'ì¤‘ìŠµ'\n",
    "    else:\n",
    "        return 'ê³ ìŠµ'\n",
    "\n",
    "def categorize_rainfall(rainfall):\n",
    "    if rainfall == 0:\n",
    "        return 'ë¹„ì—†ìŒ'\n",
    "    elif rainfall <= 4:\n",
    "        return 'ì•½í•œë¹„'\n",
    "    elif rainfall <= 9:\n",
    "        return 'ë³´í†µë¹„'\n",
    "    elif rainfall <= 29:\n",
    "        return 'ê°•í•œë¹„'\n",
    "    else:\n",
    "        return 'ë§¤ìš°ê°•í•œë¹„'\n",
    "\n",
    "def categorize_wind_speed(windspeed):\n",
    "    if windspeed <= 3:\n",
    "        return 'ì•½í’'\n",
    "    elif windspeed <= 7:\n",
    "        return 'ë³´í†µí’'\n",
    "    else:\n",
    "        return 'ê°•í’'\n",
    "\n",
    "def categorize_solar_radiation(solarradiation):\n",
    "    if solarradiation <= 10:\n",
    "        return 'ì•½í•¨'\n",
    "    elif solarradiation <= 20:\n",
    "        return 'ë³´í†µ'\n",
    "    elif solarradiation <= 25:\n",
    "        return 'ê°•í•¨'\n",
    "    else:\n",
    "        return 'ë§¤ìš°ê°•í•¨'\n",
    "\n",
    "def categorize_apparent_temp(apparenttemp):\n",
    "    if apparenttemp <= -10:\n",
    "        return 'ë§¤ìš°ì¶”ì›€'\n",
    "    elif apparenttemp <= 0:\n",
    "        return 'ì¶”ì›€'\n",
    "    elif apparenttemp <= 15:\n",
    "        return 'ìŒ€ìŒ€í•¨'\n",
    "    elif apparenttemp <= 25:\n",
    "        return 'ì¾Œì í•¨'\n",
    "    elif apparenttemp <= 32:\n",
    "        return 'ë”ì›€'\n",
    "    elif apparenttemp <= 40:\n",
    "        return 'ë¬´ë”ì›€'\n",
    "    else:\n",
    "        return 'ìœ„í—˜'\n",
    "\n",
    "# ëª¨ë“  ê¸°í›„ë³€ìˆ˜ì— ë¶„ë¥˜ ì ìš©\n",
    "df_valid['temp_category'] = df_valid['ta'].apply(categorize_temperature)\n",
    "df_valid['humidity_category'] = df_valid['hm'].apply(categorize_humidity)\n",
    "df_valid['rainfall_category'] = df_valid['rn_hr1'].apply(categorize_rainfall)\n",
    "df_valid['wind_category'] = df_valid['ws'].apply(categorize_wind_speed)\n",
    "df_valid['solar_category'] = df_valid['si'].apply(categorize_solar_radiation)\n",
    "df_valid['apparent_temp_category'] = df_valid['ta_chi'].apply(categorize_apparent_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. ë°ì´í„°ì…‹ë³„ ë¹„êµ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ë³„ ë¹„êµ ë¶„ì„\n",
    "print(\"ğŸ” ë°ì´í„°ì…‹ë³„ ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "datasets = {\n",
    "    'Complete': df_complete,\n",
    "    'Basic': df_basic, \n",
    "    'Interpolated': df_filled\n",
    "}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\nğŸ“Š {name} Dataset:\")\n",
    "        print(f\"  ë°ì´í„° ìˆ˜: {len(dataset):,}ê°œ\")\n",
    "        print(f\"  í‰ê·  í˜¼ì¡ë„: {dataset['congestion'].mean():.2f}\")\n",
    "        print(f\"  ì‹œê°„ëŒ€ ë¶„í¬: {dataset['m_hr1'].nunique()}ê°œ ì‹œê°„ëŒ€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. ë¶„ì„ìš© ë°ì´í„°ì…‹ ì„ íƒ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ì„ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì„ íƒ\n",
    "print(\"ğŸ“‹ ë¶„ì„ ë°ì´í„°ì…‹ ì„ íƒ\")\n",
    "ANALYSIS_DATASET = 'basic'  # 'complete', 'basic', 'interpolated' ì¤‘ ì„ íƒ\n",
    "\n",
    "# ê¸°ì¡´ df_validë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ìš© ë°ì´í„° í•„í„°ë§\n",
    "if ANALYSIS_DATASET == 'complete':\n",
    "    # ëª¨ë“  ê¸°í›„ë³€ìˆ˜ê°€ ì™„ì „í•œ í–‰ë§Œ ì„ íƒ\n",
    "    complete_mask = df_valid[['ta', 'ws', 'rn_hr1', 'hm', 'si', 'ta_chi']].notna().all(axis=1)\n",
    "    df_analysis = df_valid[complete_mask].copy()\n",
    "    weather_vars_analysis = ['ta', 'ws', 'rn_hr1', 'hm', 'si', 'ta_chi']\n",
    "elif ANALYSIS_DATASET == 'basic':\n",
    "    # ê¸°ë³¸ ê¸°í›„ë³€ìˆ˜ë§Œ ì™„ì „í•œ í–‰ ì„ íƒ (ì¼ì‚¬ëŸ‰ ì œì™¸)\n",
    "    basic_mask = df_valid[['ta', 'ws', 'rn_hr1', 'hm']].notna().all(axis=1)\n",
    "    df_analysis = df_valid[basic_mask].copy()\n",
    "    weather_vars_analysis = ['ta', 'ws', 'rn_hr1', 'hm']\n",
    "elif ANALYSIS_DATASET == 'interpolated':\n",
    "    # ë³´ê°„ëœ ë°ì´í„°ê°€ ìˆëŠ” í–‰ ì„ íƒ\n",
    "    if 'ta_interpolated' in df_valid.columns:\n",
    "        interpolated_mask = df_valid[['ta_interpolated', 'ws_interpolated', 'rn_hr1_filled', 'hm_interpolated']].notna().all(axis=1)\n",
    "        df_analysis = df_valid[interpolated_mask].copy()\n",
    "        weather_vars_analysis = ['ta_interpolated', 'ws_interpolated', 'rn_hr1_filled', 'hm_interpolated']\n",
    "    else:\n",
    "        print(\"âš ï¸ ë³´ê°„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        basic_mask = df_valid[['ta', 'ws', 'rn_hr1', 'hm']].notna().all(axis=1)\n",
    "        df_analysis = df_valid[basic_mask].copy()\n",
    "        weather_vars_analysis = ['ta', 'ws', 'rn_hr1', 'hm']\n",
    "\n",
    "print(f\"ì„ íƒëœ ë°ì´í„°ì…‹: {ANALYSIS_DATASET.upper()} ({len(df_analysis):,}ê°œ)\")\n",
    "print(f\"ë¶„ì„ ë³€ìˆ˜: {weather_vars_analysis}\")\n",
    "\n",
    "# df_validë¥¼ df_analysisë¡œ ì—…ë°ì´íŠ¸\n",
    "df_valid = df_analysis.copy()\n",
    "\n",
    "# í•„ìš”í•œ ë²”ì£¼í™” ì»¬ëŸ¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±\n",
    "required_categories = ['temp_category', 'humidity_category', 'rainfall_category', 'wind_category']\n",
    "for cat in required_categories:\n",
    "    if cat not in df_valid.columns:\n",
    "        if cat == 'temp_category':\n",
    "            col_name = 'ta_interpolated' if ANALYSIS_DATASET == 'interpolated' and 'ta_interpolated' in df_valid.columns else 'ta'\n",
    "            df_valid['temp_category'] = df_valid[col_name].apply(categorize_temperature)\n",
    "        elif cat == 'humidity_category':\n",
    "            col_name = 'hm_interpolated' if ANALYSIS_DATASET == 'interpolated' and 'hm_interpolated' in df_valid.columns else 'hm'\n",
    "            df_valid['humidity_category'] = df_valid[col_name].apply(categorize_humidity)\n",
    "        elif cat == 'rainfall_category':\n",
    "            col_name = 'rn_hr1_filled' if ANALYSIS_DATASET == 'interpolated' and 'rn_hr1_filled' in df_valid.columns else 'rn_hr1'\n",
    "            df_valid['rainfall_category'] = df_valid[col_name].apply(categorize_rainfall)\n",
    "        elif cat == 'wind_category':\n",
    "            col_name = 'ws_interpolated' if ANALYSIS_DATASET == 'interpolated' and 'ws_interpolated' in df_valid.columns else 'ws'\n",
    "            df_valid['wind_category'] = df_valid[col_name].apply(categorize_wind_speed)\n",
    "\n",
    "# ì„ íƒì  ë²”ì£¼í™” (ì¼ì‚¬ëŸ‰, ì²´ê°ì˜¨ë„)\n",
    "if 'si' in df_valid.columns and 'solar_category' not in df_valid.columns:\n",
    "    df_valid['solar_category'] = df_valid['si'].apply(categorize_solar_radiation)\n",
    "if 'ta_chi' in df_valid.columns and 'apparent_temp_category' not in df_valid.columns:\n",
    "    df_valid['apparent_temp_category'] = df_valid['ta_chi'].apply(categorize_apparent_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. ì‹œê°„ëŒ€ë³„ ê¸°í›„ë³€ìˆ˜-í˜¼ì¡ë„ ìƒê´€ê´€ê³„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ëŒ€ë³„ ê¸°í›„ë³€ìˆ˜ì™€ í˜¼ì¡ë„ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "weather_vars = weather_vars_analysis\n",
    "weather_categories = ['temp_category', 'wind_category', 'rainfall_category', \n",
    "                     'humidity_category', 'solar_category', 'apparent_temp_category']\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for time_group in time_order:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ• {time_group} ì‹œê°„ëŒ€ ê¸°í›„-í˜¼ì¡ë„ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # í•´ë‹¹ ì‹œê°„ëŒ€ ë°ì´í„° í•„í„°ë§\n",
    "    time_data = df_valid[df_valid['time_group'] == time_group].copy()\n",
    "    \n",
    "    if len(time_data) == 0:\n",
    "        print(\"ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ë°ì´í„° ìˆ˜: {len(time_data):,}ê°œ\")\n",
    "    \n",
    "    # 1) ì—°ì†í˜• ë³€ìˆ˜ì™€ í˜¼ì¡ë„ ê°„ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜\n",
    "    print(f\"\\nğŸ“ˆ ì—°ì†í˜• ê¸°í›„ë³€ìˆ˜ì™€ í˜¼ì¡ë„ ìƒê´€ê³„ìˆ˜:\")\n",
    "    for var in weather_vars:\n",
    "        clean_data = time_data[[var, 'congestion']].dropna()\n",
    "        if len(clean_data) > 1:\n",
    "            correlation = clean_data[var].corr(clean_data['congestion'])\n",
    "            print(f\"{var}: {correlation:.4f}\")\n",
    "        else:\n",
    "            print(f\"{var}: ë°ì´í„° ë¶€ì¡±\")\n",
    "    \n",
    "    # 2) ë²”ì£¼í˜• ë³€ìˆ˜ë³„ í˜¼ì¡ë„ í‰ê·  ë¹„êµ\n",
    "    print(f\"\\nğŸ“Š ë²”ì£¼ë³„ í‰ê·  í˜¼ì¡ë„:\")\n",
    "    for i, category in enumerate(weather_categories):\n",
    "        if category in time_data.columns:\n",
    "            category_analysis = time_data.groupby(category)['congestion'].agg(['count', 'mean', 'std'])\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(category_analysis.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. ì‹œê°„ëŒ€ë³„ ê¸°í›„ì¡°ê±´ë³„ í˜¼ì¡ë„ íˆíŠ¸ë§µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ëŒ€ë³„ ê¸°í›„ì¡°ê±´ë³„ í‰ê·  í˜¼ì¡ë„ íˆíŠ¸ë§µ\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# ê° ê¸°í›„ë³€ìˆ˜ë³„ ìˆœì„œ ì •ì˜\n",
    "category_orders = {\n",
    "    'temp_category': ['ì €ì˜¨', 'ì¤‘ì˜¨', 'ê³ ì˜¨'],\n",
    "    'humidity_category': ['ì €ìŠµ', 'ì¤‘ìŠµ', 'ê³ ìŠµ'],\n",
    "    'rainfall_category': ['ì•½í•œë¹„', 'ë³´í†µë¹„', 'ê°•í•œë¹„', 'ë§¤ìš°ê°•í•œë¹„'],\n",
    "    'wind_category': ['ì•½í’', 'ë³´í†µí’', 'ê°•í’'],\n",
    "    'solar_category': ['ì•½í•¨', 'ë³´í†µ', 'ê°•í•¨', 'ë§¤ìš°ê°•í•¨'],\n",
    "    'apparent_temp_category': ['ë§¤ìš°ì¶”ì›€', 'ì¶”ì›€', 'ìŒ€ìŒ€í•¨', 'ì¾Œì í•¨', 'ë”ì›€', 'ë¬´ë”ì›€', 'ìœ„í—˜']\n",
    "}\n",
    "\n",
    "# ê° ê¸°í›„ë³€ìˆ˜ë³„ë¡œ ì„œë¸Œí”Œë¡¯ ìƒì„±\n",
    "weather_category_pairs = [\n",
    "    ('temp_category', 'ê¸°ì˜¨'),\n",
    "    ('humidity_category', 'ìŠµë„'), \n",
    "    ('rainfall_category', 'ê°•ìˆ˜ëŸ‰'),\n",
    "    ('wind_category', 'í’ì†'),\n",
    "    ('solar_category', 'ì¼ì‚¬ëŸ‰'),\n",
    "    ('apparent_temp_category', 'ì²´ê°ì˜¨ë„')\n",
    "]\n",
    "\n",
    "for i, (category, title) in enumerate(weather_category_pairs):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # í”¼ë²— í…Œì´ë¸” ìƒì„±\n",
    "    pivot_data = df_valid.pivot_table(\n",
    "        values='congestion',\n",
    "        index='time_group_ordered',\n",
    "        columns=category,\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬ (í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê°’ë“¤ë§Œ)\n",
    "    if category in category_orders:\n",
    "        existing_cols = [col for col in category_orders[category] if col in pivot_data.columns]\n",
    "        pivot_data = pivot_data[existing_cols]\n",
    "    \n",
    "    # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°\n",
    "    sns.heatmap(pivot_data, annot=True, cmap='YlOrRd', fmt='.1f', cbar=True)\n",
    "    plt.title(f'ì‹œê°„ëŒ€ë³„ {title} êµ¬ê°„ë³„ í‰ê·  í˜¼ì¡ë„')\n",
    "    plt.xlabel(f'{title} êµ¬ê°„')\n",
    "    plt.ylabel('ì‹œê°„ëŒ€')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ (ANOVA & ìƒê´€ê³„ìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway, pearsonr\n",
    "\n",
    "# ìƒê´€ê³„ìˆ˜ ê°•ë„ í•´ì„ í•¨ìˆ˜\n",
    "def interpret_correlation(r):\n",
    "    abs_r = abs(r)\n",
    "    if abs_r < 0.20:\n",
    "        return \"ë§¤ìš° ì•½í•¨ (ë¬´ì‹œ ê°€ëŠ¥)\"\n",
    "    elif abs_r < 0.40:\n",
    "        return \"ì•½í•œ ìƒê´€\"\n",
    "    elif abs_r < 0.60:\n",
    "        return \"ì¤‘ê°„ ì •ë„ì˜ ìƒê´€\"\n",
    "    elif abs_r < 0.80:\n",
    "        return \"ê°•í•œ ìƒê´€\"\n",
    "    else:\n",
    "        return \"ë§¤ìš° ê°•í•œ ìƒê´€\"\n",
    "\n",
    "# ê° ì‹œê°„ëŒ€ë³„ë¡œ ê¸°í›„ì¡°ê±´ ê°„ í˜¼ì¡ë„ ì°¨ì´ ìœ ì˜ì„± ê²€ì •\n",
    "print(\"\\nğŸ”¬ í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ANOVA & ìƒê´€ë¶„ì„)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for time_group in time_order:\n",
    "    time_data = df_valid[df_valid['time_group'] == time_group]\n",
    "    \n",
    "    if len(time_data) == 0:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nğŸ“ {time_group}:\")\n",
    "    \n",
    "    # 1) ì—°ì†í˜• ë³€ìˆ˜ì™€ í˜¼ì¡ë„ ê°„ ìƒê´€ë¶„ì„\n",
    "    print(f\"  ğŸ”— ìƒê´€ë¶„ì„ (ì—°ì†í˜• ë³€ìˆ˜):\")\n",
    "    for var in weather_vars:\n",
    "        clean_data = time_data[[var, 'congestion']].dropna()\n",
    "        if len(clean_data) > 2:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ ë°ì´í„° í•„ìš”\n",
    "            try:\n",
    "                correlation, p_value = pearsonr(clean_data[var], clean_data['congestion'])\n",
    "                interpretation = interpret_correlation(correlation)\n",
    "                significance = \"**ìœ ì˜í•¨**\" if p_value < 0.05 else \"ìœ ì˜í•˜ì§€ ì•ŠìŒ\"\n",
    "                notable = \"â­ì£¼ëª©í• ë§Œí•¨â­\" if abs(correlation) >= 0.4 else \"\"\n",
    "                \n",
    "                print(f\"    {var}: r={correlation:.4f}, p={p_value:.4f} ({significance}) - {interpretation} {notable}\")\n",
    "            except:\n",
    "                print(f\"    {var}: ìƒê´€ë¶„ì„ ë¶ˆê°€\")\n",
    "        else:\n",
    "            print(f\"    {var}: ë°ì´í„° ë¶€ì¡± (n={len(clean_data)})\")\n",
    "    \n",
    "    # 2) ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•œ ANOVA ê²€ì •\n",
    "    print(f\"  ğŸ“Š ë¶„ì‚°ë¶„ì„ (ë²”ì£¼í˜• ë³€ìˆ˜):\")\n",
    "    for category in weather_categories:\n",
    "        if category in time_data.columns:\n",
    "            # ê° ë²”ì£¼ë³„ í˜¼ì¡ë„ ë°ì´í„° ì¶”ì¶œ\n",
    "            groups = []\n",
    "            group_info = []\n",
    "            for cat_value in time_data[category].dropna().unique():\n",
    "                group_data = time_data[time_data[category] == cat_value]['congestion'].dropna()\n",
    "                if len(group_data) > 0:\n",
    "                    groups.append(group_data)\n",
    "                    group_info.append(f\"{cat_value}(n={len(group_data)})\")\n",
    "            \n",
    "            # ANOVA ê²€ì • (2ê°œ ì´ìƒ ê·¸ë£¹ì´ ìˆì„ ë•Œ)\n",
    "            if len(groups) >= 2:\n",
    "                try:\n",
    "                    f_stat, p_value = f_oneway(*groups)\n",
    "                    significance = \"**ìœ ì˜í•¨**\" if p_value < 0.05 else \"ìœ ì˜í•˜ì§€ ì•ŠìŒ\"\n",
    "                    print(f\"    {category}: F={f_stat:.3f}, p={p_value:.4f} ({significance}) - ê·¸ë£¹: {', '.join(group_info)}\")\n",
    "                except:\n",
    "                    print(f\"    {category}: ê²€ì • ë¶ˆê°€\")\n",
    "            else:\n",
    "                print(f\"    {category}: ê·¸ë£¹ ìˆ˜ ë¶€ì¡± ({len(groups)}ê°œ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. ì‹œê°„ëŒ€ë³„ ìƒê´€ê´€ê³„ ìš”ì•½ (ë„ˆë¬´ ë‚®ì•„ì„œ ì˜ë¯¸ ì—†ìŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ëŒ€ë³„ ê°€ì¥ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ê¸°í›„ë³€ìˆ˜ ìš”ì•½\n",
    "print(\"\\nğŸ“‹ ì‹œê°„ëŒ€ë³„ ê¸°í›„ë³€ìˆ˜-í˜¼ì¡ë„ ìƒê´€ê´€ê³„ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "for time_group in time_order:\n",
    "    time_data = df_valid[df_valid['time_group'] == time_group]\n",
    "    \n",
    "    if len(time_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    correlations = {}\n",
    "    for var in weather_vars:\n",
    "        clean_data = time_data[[var, 'congestion']].dropna()\n",
    "        if len(clean_data) > 1:\n",
    "            correlations[var] = abs(clean_data[var].corr(clean_data['congestion']))\n",
    "    \n",
    "    if correlations:\n",
    "        strongest_var = max(correlations, key=correlations.get)\n",
    "        strongest_corr = correlations[strongest_var]\n",
    "        \n",
    "        summary_results.append({\n",
    "            'ì‹œê°„ëŒ€': time_group,\n",
    "            'ê°€ì¥_ê°•í•œ_ìƒê´€ë³€ìˆ˜': strongest_var,\n",
    "            'ìƒê´€ê³„ìˆ˜': strongest_corr,\n",
    "            'ë°ì´í„°_ìˆ˜': len(time_data)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. ê³„ì ˆ ì •ë³´ ì¶”ì¶œ ë° ë¶„í¬ í™•ì¸ (ê³„ì ˆë³„ í™•ì¸í•´ë³´ê³  ì‹¶ì–´ì„œ ì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³„ì ˆ ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def categorize_season(month):\n",
    "    \"\"\"ì›”ì„ ê³„ì ˆë¡œ ë¶„ë¥˜\"\"\"\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'ë´„'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'ì—¬ë¦„'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'ê°€ì„'\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 'ê²¨ìš¸'\n",
    "    else:\n",
    "        return 'ê¸°íƒ€'\n",
    "\n",
    "# ì›”ê³¼ ê³„ì ˆ ì •ë³´ ì¶”ì¶œ (ìˆ˜ì •ë¨)\n",
    "print(\"ğŸ“… ê³„ì ˆ ì •ë³´ ì¶”ì¶œ ì¤‘...\")\n",
    "\n",
    "# tm êµ¬ì¡° í™•ì¸\n",
    "print(f\"tm ìƒ˜í”Œ ê°’ë“¤: {df_valid['tm'].head().tolist()}\")\n",
    "\n",
    "# ì˜¬ë°”ë¥¸ ì›” ì¶”ì¶œ (YYYYMMDDHHì—ì„œ MM ë¶€ë¶„)\n",
    "df_valid['month'] = (df_valid['tm'] // 10000) % 100  # ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "df_valid['season'] = df_valid['month'].apply(categorize_season)\n",
    "\n",
    "# ì¶”ì¶œ ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì¶”ì¶œëœ ì›” ë²”ìœ„: {df_valid['month'].min()} ~ {df_valid['month'].max()}\")\n",
    "print(f\"ê³ ìœ  ì›” ê°’ë“¤: {sorted(df_valid['month'].unique())}\")\n",
    "\n",
    "# ê³„ì ˆë³„ ë°ì´í„° ë¶„í¬ í™•ì¸\n",
    "print(\"\\nğŸŒ¸ ê³„ì ˆë³„ ë°ì´í„° ë¶„í¬:\")\n",
    "season_counts = df_valid['season'].value_counts()\n",
    "for season, count in season_counts.items():\n",
    "    print(f\"{season}: {count:,}ê°œ ({count/len(df_valid)*100:.1f}%)\")\n",
    "\n",
    "# ì›”ë³„ ë°ì´í„° ë¶„í¬\n",
    "print(\"\\nğŸ“Š ì›”ë³„ ë°ì´í„° ë¶„í¬:\")\n",
    "month_counts = df_valid['month'].value_counts().sort_index()\n",
    "for month, count in month_counts.items():\n",
    "    season = categorize_season(month)\n",
    "    print(f\"{month:2d}ì›” ({season}): {count:,}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. ê³„ì ˆë³„ ê¸°í›„-í˜¼ì¡ë„ ìƒê´€ê´€ê³„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³„ì ˆë³„ ê¸°í›„-í˜¼ì¡ë„ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "print(\"ğŸŒ¿ ê³„ì ˆë³„ ê¸°í›„-í˜¼ì¡ë„ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "seasons = ['ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']\n",
    "weather_vars_for_corr = weather_vars_analysis  # ì„ íƒëœ ë¶„ì„ ë³€ìˆ˜ ì‚¬ìš©\n",
    "\n",
    "# ê³„ì ˆë³„ ìƒê´€ê³„ìˆ˜ ì €ì¥\n",
    "seasonal_correlations = {}\n",
    "\n",
    "for season in seasons:\n",
    "    print(f\"\\nğŸŒ¸ {season} ê³„ì ˆ ë¶„ì„:\")\n",
    "    season_data = df_valid[df_valid['season'] == season]\n",
    "    \n",
    "    if len(season_data) == 0:\n",
    "        print(f\"  âš ï¸ {season} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"  ë°ì´í„° ìˆ˜: {len(season_data):,}ê°œ\")\n",
    "    print(f\"  í‰ê·  í˜¼ì¡ë„: {season_data['congestion'].mean():.2f}\")\n",
    "    \n",
    "    # ê³„ì ˆë³„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°\n",
    "    season_corr = {}\n",
    "    for var in weather_vars_for_corr:\n",
    "        if var in season_data.columns:\n",
    "            # ê²°ì¸¡ì¹˜ ì œê±° í›„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°\n",
    "            clean_data = season_data[[var, 'congestion']].dropna()\n",
    "            if len(clean_data) > 10:  # ìµœì†Œ 10ê°œ ì´ìƒ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ\n",
    "                corr = clean_data[var].corr(clean_data['congestion'])\n",
    "                season_corr[var] = corr\n",
    "                # í†µê³„ì  ìœ ì˜ì„± í™•ì¸\n",
    "                from scipy.stats import pearsonr\n",
    "                _, p_value = pearsonr(clean_data[var], clean_data['congestion'])\n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "                print(f\"    {var}: {corr:.3f}{significance}\")\n",
    "            else:\n",
    "                season_corr[var] = np.nan\n",
    "                print(f\"    {var}: ë°ì´í„° ë¶€ì¡±\")\n",
    "    \n",
    "    seasonal_correlations[season] = season_corr\n",
    "\n",
    "# ê³„ì ˆë³„ ìƒê´€ê³„ìˆ˜ ìš”ì•½í‘œ\n",
    "print(f\"\\nğŸ“‹ ê³„ì ˆë³„ ìƒê´€ê³„ìˆ˜ ìš”ì•½:\")\n",
    "import pandas as pd\n",
    "corr_df = pd.DataFrame(seasonal_correlations).round(3)\n",
    "print(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. ê³„ì ˆë³„ ìƒê´€ê´€ê³„ ì¢…í•© ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³„ì ˆë³„ ìƒê´€ê´€ê³„ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('ê³„ì ˆë³„ ê¸°í›„ë³€ìˆ˜ì™€ ì§€í•˜ì²  í˜¼ì¡ë„ ê´€ê³„', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1) ê³„ì ˆë³„ ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ\n",
    "ax = axes[0, 0]\n",
    "if not corr_df.empty:\n",
    "    sns.heatmap(corr_df, annot=True, cmap='RdBu_r', center=0, \n",
    "                fmt='.3f', ax=ax, cbar_kws={'shrink': 0.8})\n",
    "ax.set_title('ê³„ì ˆë³„ ìƒê´€ê³„ìˆ˜')\n",
    "ax.set_xlabel('ê³„ì ˆ')\n",
    "ax.set_ylabel('ê¸°í›„ë³€ìˆ˜')\n",
    "\n",
    "# 2) ê³„ì ˆë³„ í‰ê·  í˜¼ì¡ë„\n",
    "ax = axes[0, 1]\n",
    "season_congestion = df_valid.groupby('season')['congestion'].agg(['mean', 'std']).reset_index()\n",
    "bars = ax.bar(season_congestion['season'], season_congestion['mean'], \n",
    "              yerr=season_congestion['std'], capsize=5, alpha=0.7)\n",
    "ax.set_title('ê³„ì ˆë³„ í‰ê·  í˜¼ì¡ë„')\n",
    "ax.set_ylabel('í˜¼ì¡ë„')\n",
    "ax.set_xlabel('ê³„ì ˆ')\n",
    "\n",
    "# 3) ê³„ì ˆë³„ ê¸°ì˜¨ê³¼ í˜¼ì¡ë„ ê´€ê³„\n",
    "ax = axes[0, 2]\n",
    "temp_var = 'ta_interpolated' if 'ta_interpolated' in df_valid.columns else 'ta'\n",
    "for season in seasons:\n",
    "    season_data = df_valid[df_valid['season'] == season]\n",
    "    if len(season_data) > 0:\n",
    "        ax.scatter(season_data[temp_var], season_data['congestion'], \n",
    "                  alpha=0.3, label=season, s=10)\n",
    "ax.set_xlabel('ê¸°ì˜¨ (Â°C)')\n",
    "ax.set_ylabel('í˜¼ì¡ë„')\n",
    "ax.set_title('ê³„ì ˆë³„ ê¸°ì˜¨-í˜¼ì¡ë„ ê´€ê³„')\n",
    "ax.legend()\n",
    "\n",
    "# 4) ê³„ì ˆë³„ ìŠµë„ì™€ í˜¼ì¡ë„ ê´€ê³„\n",
    "ax = axes[1, 0]\n",
    "humidity_var = 'hm_interpolated' if 'hm_interpolated' in df_valid.columns else 'hm'\n",
    "for season in seasons:\n",
    "    season_data = df_valid[df_valid['season'] == season]\n",
    "    if len(season_data) > 0:\n",
    "        ax.scatter(season_data[humidity_var], season_data['congestion'], \n",
    "                  alpha=0.3, label=season, s=10)\n",
    "ax.set_xlabel('ìŠµë„ (%)')\n",
    "ax.set_ylabel('í˜¼ì¡ë„')\n",
    "ax.set_title('ê³„ì ˆë³„ ìŠµë„-í˜¼ì¡ë„ ê´€ê³„')\n",
    "ax.legend()\n",
    "\n",
    "# 5) ê³„ì ˆë³„ ì›”í‰ê·  í˜¼ì¡ë„ íŠ¸ë Œë“œ\n",
    "ax = axes[1, 1]\n",
    "monthly_congestion = df_valid.groupby(['month', 'season'])['congestion'].mean().reset_index()\n",
    "for season in seasons:\n",
    "    season_data = monthly_congestion[monthly_congestion['season'] == season]\n",
    "    if len(season_data) > 0:\n",
    "        ax.plot(season_data['month'], season_data['congestion'], \n",
    "               marker='o', label=season, linewidth=2)\n",
    "ax.set_xlabel('ì›”')\n",
    "ax.set_ylabel('í‰ê·  í˜¼ì¡ë„')\n",
    "ax.set_title('ì›”ë³„ í‰ê·  í˜¼ì¡ë„ íŠ¸ë Œë“œ')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 13))\n",
    "\n",
    "# 6) ê³„ì ˆë³„ ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ íŒ¨í„´\n",
    "ax = axes[1, 2]\n",
    "hourly_seasonal = df_valid.groupby(['m_hr1', 'season'])['congestion'].mean().reset_index()\n",
    "for season in seasons:\n",
    "    season_data = hourly_seasonal[hourly_seasonal['season'] == season]\n",
    "    if len(season_data) > 0:\n",
    "        ax.plot(season_data['m_hr1'], season_data['congestion'], \n",
    "               marker='o', label=season, alpha=0.8)\n",
    "ax.set_xlabel('ì‹œê°„')\n",
    "ax.set_ylabel('í‰ê·  í˜¼ì¡ë„')\n",
    "ax.set_title('ê³„ì ˆë³„ ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. ê³„ì ˆë³„ ì‹¬í™” ë¶„ì„ (ê·¹ê°’, ì „í™˜ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³„ì ˆë³„ ì‹¬í™” ë¶„ì„\n",
    "print(\"ğŸ”¬ ê³„ì ˆë³„ ì‹¬í™” ë¶„ì„\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ê³„ì ˆë³„ ê·¹ê°’ ë¶„ì„\n",
    "print(\"\\nğŸŒ¡ï¸ ê³„ì ˆë³„ ê¸°í›„ ê·¹ê°’ì—ì„œì˜ í˜¼ì¡ë„:\")\n",
    "for season in seasons:\n",
    "    season_data = df_valid[df_valid['season'] == season]\n",
    "    if len(season_data) == 0:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{season} ê³„ì ˆ:\")\n",
    "    \n",
    "    # ê¸°ì˜¨ ê·¹ê°’\n",
    "    temp_var = 'ta_interpolated' if 'ta_interpolated' in season_data.columns else 'ta'\n",
    "    temp_high = season_data[season_data[temp_var] >= season_data[temp_var].quantile(0.9)]\n",
    "    temp_low = season_data[season_data[temp_var] <= season_data[temp_var].quantile(0.1)]\n",
    "    \n",
    "    print(f\"  ê³ ì˜¨(ìƒìœ„10%): í‰ê· í˜¼ì¡ë„ {temp_high['congestion'].mean():.2f}\")\n",
    "    print(f\"  ì €ì˜¨(í•˜ìœ„10%): í‰ê· í˜¼ì¡ë„ {temp_low['congestion'].mean():.2f}\")\n",
    "    \n",
    "    # ê°•ìˆ˜ëŸ‰\n",
    "    rain_var = 'rn_hr1_filled' if 'rn_hr1_filled' in season_data.columns else 'rn_hr1'\n",
    "    rainy = season_data[season_data[rain_var] > 0]\n",
    "    no_rain = season_data[season_data[rain_var] == 0]\n",
    "    \n",
    "    if len(rainy) > 0:\n",
    "        print(f\"  ë¹„ì˜¤ëŠ”ë‚ : í‰ê· í˜¼ì¡ë„ {rainy['congestion'].mean():.2f} ({len(rainy):,}ê°œ)\")\n",
    "    if len(no_rain) > 0:\n",
    "        print(f\"  ë§‘ì€ë‚ : í‰ê· í˜¼ì¡ë„ {no_rain['congestion'].mean():.2f} ({len(no_rain):,}ê°œ)\")\n",
    "\n",
    "# ê³„ì ˆ ì „í™˜ê¸° ë¶„ì„\n",
    "print(f\"\\nğŸ”„ ê³„ì ˆ ì „í™˜ê¸° í˜¼ì¡ë„ ë³€í™”:\")\n",
    "transition_months = {\n",
    "    'ë´„â†’ì—¬ë¦„': [5, 6],\n",
    "    'ì—¬ë¦„â†’ê°€ì„': [8, 9], \n",
    "    'ê°€ì„â†’ê²¨ìš¸': [11, 12],\n",
    "    'ê²¨ìš¸â†’ë´„': [2, 3]\n",
    "}\n",
    "\n",
    "for transition, months in transition_months.items():\n",
    "    month_data = []\n",
    "    for month in months:\n",
    "        month_congestion = df_valid[df_valid['month'] == month]['congestion'].mean()\n",
    "        month_data.append(month_congestion)\n",
    "    \n",
    "    if len(month_data) == 2:\n",
    "        change = month_data[1] - month_data[0]\n",
    "        print(f\"  {transition}: {change:+.2f} ({month_data[0]:.2f} â†’ {month_data[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë¬´ê²ƒë„ ìƒê´€ì´ ì—†ëŠ”ë“¯ ,,, ì–´ì¹´ì§€ ìš°ì—¥ ì ˆëŒ€ì ì¸ ì‹œê°„ëŒ€ ì œì™¸í•˜ê³  ì–´ë–»ê²Œ í•  ìˆ˜ ìˆëŠ”ì§€ ë¬¼ì–´ë´ì•¼ê² ë‹¤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
