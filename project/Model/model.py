"""
ì‹œê³„ì—´ ê²€ì¦: 2021ë…„ í›ˆë ¨ â†’ 2022ë…„ ê²€ì¦
ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì™€ ë™ì¼í•œ ê²€ì¦ ë°©ë²•
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
import warnings
import os
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

class TimeSeriesValidator:
    """2021â†’2023 ì‹œê³„ì—´ ê²€ì¦"""
    
    def __init__(self):
        self.train_data = None
        self.test_data = None
        self.models = {}
        self.results = {}
        
    def load_data(self, train_years=['21'], test_year='23', sample_size=5000000):
        """ë³µìˆ˜ í•™ìŠµ ë°ì´í„°(21,22ë…„)ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°(23ë…„) ë¡œë“œ"""
        print(f"ğŸš€ ì‹œê³„ì—´ ê²€ì¦ ë°ì´í„° ë¡œë“œ")
        print(f"  ğŸ“š í›ˆë ¨: {', '.join('20'+y for y in train_years)}ë…„ ë°ì´í„°")
        print(f"  ğŸ¯ ê²€ì¦: 20{test_year}ë…„ ë°ì´í„°")
        print("="*60)

        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            data_dir = os.path.abspath(os.path.join(base_dir, '..', 'ë°ì´í„°'))
            
            # ì—¬ëŸ¬ í•™ìŠµ ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ concat
            train_dfs = []
            for y in train_years:
                train_file = os.path.join(data_dir, f'train_subway{y}.csv')
                print(f"ğŸ“Š 20{y}ë…„ í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
                df = pd.read_csv(train_file, encoding='cp949', nrows=sample_size)
                df = self._preprocess_data(df, y)
                train_dfs.append(df)
            self.train_data = pd.concat(train_dfs).reset_index(drop=True)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            test_file = os.path.join(data_dir, f'train_subway{test_year}.csv')
            print(f"ğŸ“Š 20{test_year}ë…„ ê²€ì¦ ë°ì´í„° ë¡œë“œ ì¤‘...")
            test_df = pd.read_csv(test_file, encoding='cp949', nrows=sample_size)
            self.test_data = self._preprocess_data(test_df, test_year)

            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
            print(f"  ğŸ“š í›ˆë ¨ ë°ì´í„°: {len(self.train_data):,}ê°œ ({', '.join('20'+y for y in train_years)}ë…„)")
            print(f"  ğŸ¯ ê²€ì¦ ë°ì´í„°: {len(self.test_data):,}ê°œ (20{test_year}ë…„)")
            return True
        
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _preprocess_data(self, df, year):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        df.columns = [col.replace(f'train_subway{year}.', '') for col in df.columns]
        
        # ì‹œê°„ íŠ¹ì„± ìƒì„±
        df['datetime'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')
        df['hour'] = df['datetime'].dt.hour
        df['dayofweek'] = df['datetime'].dt.dayofweek
        df['month'] = df['datetime'].dt.month
        df['day'] = df['datetime'].dt.day
        df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
        
        # ê³„ì ˆ íŠ¹ì„±
        df['season'] = df['month'].map({
            12: 0, 1: 0, 2: 0,  # ê²¨ìš¸
            3: 1, 4: 1, 5: 1,   # ë´„
            6: 2, 7: 2, 8: 2,   # ì—¬ë¦„
            9: 3, 10: 3, 11: 3  # ê°€ì„
        })
        
        # ì¶œí‡´ê·¼ ì‹œê°„ íŠ¹ì„±
        df['is_rush_hour'] = df['hour'].apply(
            lambda x: 1 if x in [7, 8, 9, 18, 19, 20] else 0
        )
        
        # ìˆœí™˜ì  ì‹œê°„ íŠ¹ì„±
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
        df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        # ê¸°ìƒ ë°ì´í„° ì „ì²˜ë¦¬
        weather_cols = ['ta', 'ws', 'rn_hr1', 'hm']
        for col in weather_cols:
            if col in df.columns:
                # ì´ìƒì¹˜ ì²˜ë¦¬
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                df[col] = df[col].clip(Q1 - 1.5*IQR, Q3 + 1.5*IQR)
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.fillna(df.median(numeric_only=True))
        
        return df
    
    def prepare_features(self):
        """íŠ¹ì„± ì¤€ë¹„ (í›ˆë ¨/ê²€ì¦ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥)"""
        print("\nğŸ“Š íŠ¹ì„± ì¤€ë¹„ ì¤‘...")
        
        # ê¸°ë³¸ íŠ¹ì„± ì„ íƒ
        base_feature_cols = [
            'hour', 'dayofweek', 'month', 'day', 'season',
            'is_weekend', 'is_rush_hour',
            'hour_sin', 'hour_cos', 'day_sin', 'day_cos',
            'month_sin', 'month_cos',
            'ta', 'ws', 'rn_hr1', 'hm'
        ]
        
        # Lag features ìƒì„±
        print("ğŸ•’ Lag features ìƒì„± ì¤‘...")
        lag_periods = [1, 2, 3, 6, 12, 24, 168]  # 1ì‹œê°„, 2ì‹œê°„, 3ì‹œê°„, 6ì‹œê°„, 12ì‹œê°„, 1ì¼, 1ì£¼ì¼ ì „
        
        for dataset_name, dataset in [('train', self.train_data), ('test', self.test_data)]:
            print(f"  ğŸ“ˆ {dataset_name} ë°ì´í„° Lag features ìƒì„±...")
            
            # ì‹œê°„ìˆœ ì •ë ¬
            dataset = dataset.sort_values(['station_name', 'datetime']).reset_index(drop=True)
            
            # ê° ì—­ë³„ë¡œ lag features ìƒì„±
            for lag in lag_periods:
                lag_col_name = f'congestion_lag_{lag}'
                dataset[lag_col_name] = np.nan
                
                for station in dataset['station_name'].unique():
                    station_mask = dataset['station_name'] == station
                    station_data = dataset[station_mask].copy()
                    
                    # lag feature ìƒì„± (ì—­ë³„ë¡œ)
                    lagged_values = station_data['congestion'].shift(lag)
                    dataset.loc[station_mask, lag_col_name] = lagged_values
            
            # ë¡¤ë§ ìœˆë„ìš° í†µê³„ íŠ¹ì„± ì¶”ê°€
            rolling_windows = [3, 6, 12, 24]  # 3ì‹œê°„, 6ì‹œê°„, 12ì‹œê°„, 24ì‹œê°„ ë¡¤ë§
            for window in rolling_windows:
                for stat in ['mean', 'std', 'min', 'max']:
                    col_name = f'congestion_rolling_{window}h_{stat}'
                    dataset[col_name] = np.nan
                    
                    for station in dataset['station_name'].unique():
                        station_mask = dataset['station_name'] == station
                        station_data = dataset[station_mask].copy()
                        
                        if stat == 'mean':
                            rolling_values = station_data['congestion'].rolling(window=window, min_periods=1).mean().shift(1)
                        elif stat == 'std':
                            rolling_values = station_data['congestion'].rolling(window=window, min_periods=1).std().shift(1)
                        elif stat == 'min':
                            rolling_values = station_data['congestion'].rolling(window=window, min_periods=1).min().shift(1)
                        elif stat == 'max':
                            rolling_values = station_data['congestion'].rolling(window=window, min_periods=1).max().shift(1)
                        
                        dataset.loc[station_mask, col_name] = rolling_values
            
            # ì‹œê°„ëŒ€ë³„ í‰ê·  í˜¼ì¡ë„ íŠ¹ì„± (ê³¼ê±° ë°ì´í„° ê¸°ë°˜)
            hourly_avg_col = 'hourly_avg_congestion'
            dataset[hourly_avg_col] = np.nan
            
            for station in dataset['station_name'].unique():
                station_mask = dataset['station_name'] == station
                station_data = dataset[station_mask].copy()
                
                # ê° ì‹œì ì˜ ì´ì „ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ëŒ€ë³„ í‰ê·  ê³„ì‚°
                for idx in station_data.index:
                    current_hour = station_data.loc[idx, 'hour']
                    current_datetime = station_data.loc[idx, 'datetime']
                    
                    # í˜„ì¬ ì‹œì  ì´ì „ì˜ ê°™ì€ ì‹œê°„ëŒ€ ë°ì´í„°
                    historical_mask = (station_data['hour'] == current_hour) & (station_data['datetime'] < current_datetime)
                    if historical_mask.sum() > 0:
                        avg_congestion = station_data.loc[historical_mask, 'congestion'].mean()
                        dataset.loc[idx, hourly_avg_col] = avg_congestion
            
            # ì—…ë°ì´íŠ¸
            if dataset_name == 'train':
                self.train_data = dataset
            else:
                self.test_data = dataset
        
        # ì „ì²´ íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        lag_feature_cols = [f'congestion_lag_{lag}' for lag in lag_periods]
        rolling_feature_cols = [f'congestion_rolling_{window}h_{stat}' 
                               for window in rolling_windows 
                               for stat in ['mean', 'std', 'min', 'max']]
        time_avg_cols = ['hourly_avg_congestion']
        
        feature_cols = base_feature_cols + lag_feature_cols + rolling_feature_cols + time_avg_cols
        
        # ê³µí†µ íŠ¹ì„±ë§Œ ì„ íƒ
        available_features = [col for col in feature_cols 
                            if col in self.train_data.columns and col in self.test_data.columns]
        
        # ì—­ ì¸ì½”ë”© (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€)
        le_station = LabelEncoder()
        train_stations = self.train_data['station_name'].unique()
        test_stations = self.test_data['station_name'].unique()
        
        # ê³µí†µ ì—­ë§Œ ì‚¬ìš©
        common_stations = set(train_stations) & set(test_stations)
        print(f"ğŸ“ ê³µí†µ ì—­: {len(common_stations)}ê°œ")
        
        # ê³µí†µ ì—­ë§Œ í•„í„°ë§
        self.train_data = self.train_data[self.train_data['station_name'].isin(common_stations)]
        self.test_data = self.test_data[self.test_data['station_name'].isin(common_stations)]
        
        # ì—­ ì¸ì½”ë”©
        le_station.fit(sorted(common_stations))
        self.train_data['station_encoded'] = le_station.transform(self.train_data['station_name'])
        self.test_data['station_encoded'] = le_station.transform(self.test_data['station_name'])
        
        available_features.append('station_encoded')
        
        # íŠ¹ì„± ë° íƒ€ê²Ÿ ì¤€ë¹„
        X_train = self.train_data[available_features]
        y_train = self.train_data['congestion']
        X_test = self.test_data[available_features]
        y_test = self.test_data['congestion']
        
        # Lag features ë•Œë¬¸ì— ìƒê¸´ ê²°ì¸¡ê°’ ì œê±°
        print("ğŸ§¹ ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
        
        # ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ë§ì€ ì´ˆê¸° ë°ì´í„° ì œê±° (lag features ë•Œë¬¸ì—)
        max_lag = max(lag_periods)
        
        # ê° ì—­ë³„ë¡œ ì²˜ìŒ max_lagê°œ ì‹œê°„ì˜ ë°ì´í„° ì œê±°
        train_valid_indices = []
        test_valid_indices = []
        
        for station in common_stations:
            # í›ˆë ¨ ë°ì´í„°
            station_train_mask = self.train_data['station_name'] == station
            station_train_data = self.train_data[station_train_mask].sort_values('datetime')
            if len(station_train_data) > max_lag:
                valid_train_indices = station_train_data.index[max_lag:]
                train_valid_indices.extend(valid_train_indices)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            station_test_mask = self.test_data['station_name'] == station
            station_test_data = self.test_data[station_test_mask].sort_values('datetime')
            if len(station_test_data) > max_lag:
                valid_test_indices = station_test_data.index[max_lag:]
                test_valid_indices.extend(valid_test_indices)
        
        # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì„ íƒ
        X_train = X_train.loc[train_valid_indices]
        y_train = y_train.loc[train_valid_indices]
        X_test = X_test.loc[test_valid_indices]
        y_test = y_test.loc[test_valid_indices]
        
        # ë‚˜ë¨¸ì§€ ê²°ì¸¡ê°’ ì²˜ë¦¬ (interpolation)
        print("ğŸ”§ ë‚˜ë¨¸ì§€ ê²°ì¸¡ê°’ ë³´ê°„ ì²˜ë¦¬...")
        X_train = X_train.ffill().bfill()
        X_test = X_test.ffill().bfill()
        
        # ìµœì¢… ê²°ì¸¡ê°’ ì œê±°
        final_train_mask = ~(X_train.isna().any(axis=1) | y_train.isna())
        final_test_mask = ~(X_test.isna().any(axis=1) | y_test.isna())
        
        X_train = X_train[final_train_mask]
        y_train = y_train[final_train_mask]
        X_test = X_test[final_test_mask]
        y_test = y_test[final_test_mask]
        
        print(f"âœ… íŠ¹ì„± ì¤€ë¹„ ì™„ë£Œ:")
        print(f"  - ê¸°ë³¸ íŠ¹ì„±: {len(base_feature_cols)}ê°œ")
        print(f"  - Lag íŠ¹ì„±: {len(lag_feature_cols)}ê°œ")
        print(f"  - ë¡¤ë§ í†µê³„: {len(rolling_feature_cols)}ê°œ")
        print(f"  - ì‹œê°„ëŒ€ë³„ í‰ê· : {len(time_avg_cols)}ê°œ")
        print(f"  - ì´ íŠ¹ì„± ìˆ˜: {len(available_features)}ê°œ")
        print(f"  - í›ˆë ¨ ìƒ˜í”Œ: {len(X_train):,}ê°œ")
        print(f"  - ê²€ì¦ ìƒ˜í”Œ: {len(X_test):,}ê°œ")
        
        self.feature_names = available_features
        return X_train, y_train, X_test, y_test
    
    def train_and_validate(self, X_train, y_train, X_test, y_test):
        """ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦"""
        print("\nğŸš€ ì‹œê³„ì—´ ê²€ì¦ ì‹œì‘ (2021â†’2022)")
        print("-" * 60)
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ëª¨ë¸ ì •ì˜
        models = {
            #'Linear Regression': LinearRegression(),
            #'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
            'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, tree_method='gpu_hist')

        }
        
        results = {}
        
        for name, model in models.items():
            print(f"  ğŸ”„ {name} í›ˆë ¨ ì¤‘...")
            
            try:
                # ëª¨ë¸ í›ˆë ¨ (2021 ë°ì´í„°)
                if name == 'Linear Regression':
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)
                else:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                
                # 2022 ë°ì´í„°ë¡œ ê²€ì¦
                mae = mean_absolute_error(y_test, y_pred)
                rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                r2 = r2_score(y_test, y_pred)
                
                results[name] = {
                    'model': model,
                    'mae': mae,
                    'rmse': rmse,
                    'r2': r2,
                    'predictions': y_pred
                }
                
                print(f"    âœ… {name} ì™„ë£Œ - MAE: {mae:.3f}, RÂ²: {r2:.3f}")
                
            except Exception as e:
                print(f"    âŒ {name} ì‹¤íŒ¨: {str(e)}")
        
        self.models = models
        self.results = results
        self.X_test = X_test
        self.y_test = y_test
        
        return results
    
    def analyze_temporal_generalization(self):
        """ì‹œê°„ì  ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ì‹œê°„ì  ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„")
        print("="*60)
        
        # ì—°ë„ë³„ ë°ì´í„° íŠ¹ì„± ë¹„êµ
        print(f"\nğŸ“Š ì—°ë„ë³„ ë°ì´í„° íŠ¹ì„± ë¹„êµ:")
        
        train_stats = {
            'í‰ê·  í˜¼ì¡ë„': self.train_data['congestion'].mean(),
            'í˜¼ì¡ë„ í‘œì¤€í¸ì°¨': self.train_data['congestion'].std(),
            'í‰ê·  ì˜¨ë„': self.train_data['ta'].mean(),
            'í‰ê·  ìŠµë„': self.train_data['hm'].mean(),
        }
        
        test_stats = {
            'í‰ê·  í˜¼ì¡ë„': self.test_data['congestion'].mean(),
            'í˜¼ì¡ë„ í‘œì¤€í¸ì°¨': self.test_data['congestion'].std(),
            'í‰ê·  ì˜¨ë„': self.test_data['ta'].mean(),
            'í‰ê·  ìŠµë„': self.test_data['hm'].mean(),
        }
        
        print(f"{'íŠ¹ì„±':<15} {'2021ë…„':<10} {'2022ë…„':<10} {'ì°¨ì´':<10}")
        print("-" * 50)
        for key in train_stats:
            diff = test_stats[key] - train_stats[key]
            print(f"{key:<15} {train_stats[key]:<10.2f} {test_stats[key]:<10.2f} {diff:+7.2f}")
        
        # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        print(f"\nğŸ† ëª¨ë¸ë³„ ì‹œê°„ì  ì¼ë°˜í™” ì„±ëŠ¥:")
        print(f"{'ëª¨ë¸ëª…':<20} {'MAE':<10} {'RMSE':<10} {'RÂ²':<10}")
        print("-" * 55)
        
        for name, result in self.results.items():
            print(f"{name:<20} {result['mae']:<10.3f} {result['rmse']:<10.3f} {result['r2']:<10.3f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model_name = min(self.results.keys(), key=lambda x: self.results[x]['mae'])
        best_result = self.results[best_model_name]
        best_model = best_result['model']
        
        print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
        print(f"  - MAE: {best_result['mae']:.3f}")
        print(f"  - RÂ²: {best_result['r2']:.3f}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (Random Forestì¸ ê²½ìš°)
        if hasattr(best_model, 'feature_importances_'):
            print(f"\nğŸ” íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ìƒìœ„ 15ê°œ):")
            feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': best_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print(f"{'íŠ¹ì„±ëª…':<25} {'ì¤‘ìš”ë„':<10} {'ìœ í˜•':<15}")
            print("-" * 50)
            
            for idx, row in feature_importance.head(15).iterrows():
                feature_name = row['feature']
                importance = row['importance']
                
                # íŠ¹ì„± ìœ í˜• ë¶„ë¥˜
                if 'lag' in feature_name:
                    feature_type = 'Lag Feature'
                elif 'rolling' in feature_name:
                    feature_type = 'Rolling Stats'
                elif 'hourly_avg' in feature_name:
                    feature_type = 'Time Average'
                elif feature_name in ['hour', 'dayofweek', 'month', 'is_weekend', 'is_rush_hour']:
                    feature_type = 'Time Feature'
                elif feature_name in ['ta', 'ws', 'rn_hr1', 'hm']:
                    feature_type = 'Weather'
                else:
                    feature_type = 'Other'
                
                print(f"{feature_name:<25} {importance:<10.4f} {feature_type:<15}")
            
            # Lag features ì¤‘ìš”ë„ ìš”ì•½
            lag_features = feature_importance[feature_importance['feature'].str.contains('lag')]
            if len(lag_features) > 0:
                print(f"\nğŸ“Š Lag Features ì¤‘ìš”ë„ ìš”ì•½:")
                print(f"  - ì´ Lag features: {len(lag_features)}ê°œ")
                print(f"  - í‰ê·  ì¤‘ìš”ë„: {lag_features['importance'].mean():.4f}")
                print(f"  - ìµœê³  ì¤‘ìš”ë„ Lag: {lag_features.iloc[0]['feature']} ({lag_features.iloc[0]['importance']:.4f})")
                
                # ìƒìœ„ 5ê°œ lag features
                print(f"  - ìƒìœ„ 5ê°œ Lag features:")
                for idx, row in lag_features.head(5).iterrows():
                    lag_period = row['feature'].split('_')[-1]
                    print(f"    â€¢ {lag_period}ì‹œê°„ ì „: {row['importance']:.4f}")
        
        return best_model_name, best_result
    
    def plot_validation_results(self):
        """ê²€ì¦ ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“Š ê²€ì¦ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        fig, axes = plt.subplots(3, 3, figsize=(20, 16))
        
        # 1. ì—°ë„ë³„ í˜¼ì¡ë„ ë¶„í¬ ë¹„êµ
        axes[0, 0].hist(self.train_data['congestion'], bins=50, alpha=0.7, 
                       label='2021ë…„ (í›ˆë ¨)', density=True)
        axes[0, 0].hist(self.test_data['congestion'], bins=50, alpha=0.7, 
                       label='2022ë…„ (ê²€ì¦)', density=True)
        axes[0, 0].set_title('ì—°ë„ë³„ í˜¼ì¡ë„ ë¶„í¬ ë¹„êµ')
        axes[0, 0].set_xlabel('í˜¼ì¡ë„')
        axes[0, 0].set_ylabel('ë°€ë„')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        model_names = list(self.results.keys())
        mae_scores = [self.results[name]['mae'] for name in model_names]
        r2_scores = [self.results[name]['r2'] for name in model_names]
        
        x_pos = np.arange(len(model_names))
        axes[0, 1].bar(x_pos, mae_scores, alpha=0.7, color='skyblue')
        axes[0, 1].set_title('ëª¨ë¸ë³„ MAE (2022ë…„ ê²€ì¦)')
        axes[0, 1].set_xlabel('ëª¨ë¸')
        axes[0, 1].set_ylabel('MAE')
        axes[0, 1].set_xticks(x_pos)
        axes[0, 1].set_xticklabels(model_names, rotation=45)
        
        # ê°’ í‘œì‹œ
        for i, v in enumerate(mae_scores):
            axes[0, 1].text(i, v + 0.05, f'{v:.3f}', ha='center')
        
        # 3. RÂ² ì ìˆ˜
        axes[0, 2].bar(x_pos, r2_scores, alpha=0.7, color='lightgreen')
        axes[0, 2].set_title('ëª¨ë¸ë³„ RÂ² (2022ë…„ ê²€ì¦)')
        axes[0, 2].set_xlabel('ëª¨ë¸')
        axes[0, 2].set_ylabel('RÂ²')
        axes[0, 2].set_xticks(x_pos)
        axes[0, 2].set_xticklabels(model_names, rotation=45)
        
        for i, v in enumerate(r2_scores):
            axes[0, 2].text(i, v + 0.01, f'{v:.3f}', ha='center')
        
        # 4. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ ì˜ˆì¸¡ vs ì‹¤ì œ
        best_model_name = min(self.results.keys(), key=lambda x: self.results[x]['mae'])
        best_predictions = self.results[best_model_name]['predictions']
        best_model = self.results[best_model_name]['model']
        
        # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°í™”ê°€ ì–´ë ¤ì›€)
        sample_size = min(5000, len(self.y_test))
        sample_indices = np.random.choice(len(self.y_test), sample_size, replace=False)
        
        axes[1, 0].scatter(self.y_test.iloc[sample_indices], best_predictions[sample_indices], 
                          alpha=0.5, s=1)
        axes[1, 0].plot([self.y_test.min(), self.y_test.max()], 
                       [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        axes[1, 0].set_title(f'{best_model_name} - ì˜ˆì¸¡ vs ì‹¤ì œ')
        axes[1, 0].set_xlabel('ì‹¤ì œê°’ (2022)')
        axes[1, 0].set_ylabel('ì˜ˆì¸¡ê°’')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. ì”ì°¨ ë¶„ì„
        residuals = self.y_test.iloc[sample_indices] - best_predictions[sample_indices]
        axes[1, 1].scatter(best_predictions[sample_indices], residuals, alpha=0.5, s=1)
        axes[1, 1].axhline(y=0, color='r', linestyle='--')
        axes[1, 1].set_title('ì”ì°¨ ë¶„ì„')
        axes[1, 1].set_xlabel('ì˜ˆì¸¡ê°’')
        axes[1, 1].set_ylabel('ì”ì°¨')
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. ì‹œê°„ë³„ ì„±ëŠ¥ ë¶„ì„
        # 2022ë…„ ë°ì´í„°ì— ì˜ˆì¸¡ê°’ ì¶”ê°€
        test_with_pred = self.test_data.copy()
        test_with_pred['predictions'] = best_predictions
        test_with_pred['residuals'] = abs(test_with_pred['congestion'] - test_with_pred['predictions'])
        
        hourly_mae = test_with_pred.groupby('hour')['residuals'].mean()
        axes[1, 2].plot(hourly_mae.index, hourly_mae.values, marker='o', linewidth=2)
        axes[1, 2].set_title('ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ ì˜¤ì°¨ (MAE)')
        axes[1, 2].set_xlabel('ì‹œê°„')
        axes[1, 2].set_ylabel('í‰ê·  ì ˆëŒ€ ì˜¤ì°¨')
        axes[1, 2].grid(True, alpha=0.3)
        axes[1, 2].set_xticks(range(0, 24, 2))
        
        # 7. íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)
        if hasattr(best_model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': best_model.feature_importances_
            }).sort_values('importance', ascending=False).head(10)
            
            y_pos = np.arange(len(feature_importance))
            axes[2, 0].barh(y_pos, feature_importance['importance'], alpha=0.7)
            axes[2, 0].set_yticks(y_pos)
            axes[2, 0].set_yticklabels(feature_importance['feature'], fontsize=8)
            axes[2, 0].set_xlabel('ì¤‘ìš”ë„')
            axes[2, 0].set_title('íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)')
            axes[2, 0].grid(True, alpha=0.3)
            
            # 8. Lag featuresë³„ ì¤‘ìš”ë„
            lag_features = feature_importance[feature_importance['feature'].str.contains('lag')]
            if len(lag_features) > 0:
                lag_periods = [int(feat.split('_')[-1]) for feat in lag_features['feature']]
                axes[2, 1].bar(range(len(lag_periods)), lag_features['importance'], alpha=0.7, color='orange')
                axes[2, 1].set_title('Lag Features ì¤‘ìš”ë„')
                axes[2, 1].set_xlabel('Lag Period (ì‹œê°„)')
                axes[2, 1].set_ylabel('ì¤‘ìš”ë„')
                axes[2, 1].set_xticks(range(len(lag_periods)))
                axes[2, 1].set_xticklabels([f'{p}h' for p in lag_periods], rotation=45)
                axes[2, 1].grid(True, alpha=0.3)
            else:
                axes[2, 1].text(0.5, 0.5, 'Lag Features ì—†ìŒ', ha='center', va='center', transform=axes[2, 1].transAxes)
                axes[2, 1].set_title('Lag Features ì¤‘ìš”ë„')
        
        # 9. íŠ¹ì„± ìœ í˜•ë³„ ì¤‘ìš”ë„ í•©ê³„
        if hasattr(best_model, 'feature_importances_'):
            feature_importance_full = pd.DataFrame({
                'feature': self.feature_names,
                'importance': best_model.feature_importances_
            })
            
            # íŠ¹ì„± ìœ í˜•ë³„ ë¶„ë¥˜
            feature_types = []
            for feature in feature_importance_full['feature']:
                if 'lag' in feature:
                    feature_types.append('Lag Features')
                elif 'rolling' in feature:
                    feature_types.append('Rolling Stats')
                elif 'hourly_avg' in feature:
                    feature_types.append('Time Average')
                elif feature in ['hour', 'dayofweek', 'month', 'is_weekend', 'is_rush_hour', 'season']:
                    feature_types.append('Time Features')
                elif feature in ['ta', 'ws', 'rn_hr1', 'hm']:
                    feature_types.append('Weather')
                elif 'sin' in feature or 'cos' in feature:
                    feature_types.append('Cyclic Features')
                else:
                    feature_types.append('Other')
            
            feature_importance_full['type'] = feature_types
            type_importance = feature_importance_full.groupby('type')['importance'].sum().sort_values(ascending=False)
            
            axes[2, 2].pie(type_importance.values, labels=type_importance.index, autopct='%1.1f%%', startangle=90)
            axes[2, 2].set_title('íŠ¹ì„± ìœ í˜•ë³„ ì¤‘ìš”ë„ ë¹„ìœ¨')
        
        plt.tight_layout()
        plt.savefig('../result/time_series_validation_with_lag.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_insights(self):
        """ì‹œê³„ì—´ ê²€ì¦ ì¸ì‚¬ì´íŠ¸"""
        print("\n" + "="*60)
        print("ğŸ” ì‹œê³„ì—´ ê²€ì¦ ì¸ì‚¬ì´íŠ¸")
        print("="*60)
        
        best_model_name = min(self.results.keys(), key=lambda x: self.results[x]['mae'])
        best_result = self.results[best_model_name]
        best_model = best_result['model']
        
        print(f"\nğŸ¯ ì‹œê°„ì  ì¼ë°˜í™” ì„±ëŠ¥:")
        print(f"  - ìµœê³  ëª¨ë¸: {best_model_name}")
        print(f"  - 2022ë…„ ì˜ˆì¸¡ MAE: {best_result['mae']:.3f}")
        print(f"  - 2022ë…„ ì˜ˆì¸¡ RÂ²: {best_result['r2']:.3f}")
        
        # ì„±ëŠ¥ í‰ê°€
        if best_result['r2'] > 0.8:
            performance = "ìš°ìˆ˜"
        elif best_result['r2'] > 0.6:
            performance = "ì–‘í˜¸"
        else:
            performance = "ê°œì„  í•„ìš”"
        
        print(f"\nğŸ“Š ì„±ëŠ¥ í‰ê°€: {performance}")
        
        # Lag features ë¶„ì„
        if hasattr(best_model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': best_model.feature_importances_
            })
            
            # Lag features ì¤‘ìš”ë„ ë¶„ì„
            lag_features = feature_importance[feature_importance['feature'].str.contains('lag')]
            rolling_features = feature_importance[feature_importance['feature'].str.contains('rolling')]
            time_avg_features = feature_importance[feature_importance['feature'].str.contains('hourly_avg')]
            
            total_temporal_importance = (lag_features['importance'].sum() + 
                                       rolling_features['importance'].sum() + 
                                       time_avg_features['importance'].sum())
            
            print(f"\nğŸ•’ ì‹œê°„ì  íŠ¹ì„± ë¶„ì„:")
            print(f"  - Lag Features ê°œìˆ˜: {len(lag_features)}ê°œ")
            print(f"  - Lag Features ì¤‘ìš”ë„ í•©ê³„: {lag_features['importance'].sum():.3f}")
            print(f"  - Rolling Stats ì¤‘ìš”ë„ í•©ê³„: {rolling_features['importance'].sum():.3f}")
            print(f"  - ì‹œê°„ëŒ€ë³„ í‰ê·  ì¤‘ìš”ë„: {time_avg_features['importance'].sum():.3f}")
            print(f"  - ì „ì²´ ì‹œê°„ì  íŠ¹ì„± ì¤‘ìš”ë„: {total_temporal_importance:.3f}")
            
            if len(lag_features) > 0:
                # ê°€ì¥ ì¤‘ìš”í•œ lag period ë¶„ì„
                top_lag = lag_features.iloc[0]
                lag_period = int(top_lag['feature'].split('_')[-1])
                
                if lag_period == 1:
                    lag_desc = "1ì‹œê°„ ì „ (ì§ì „ ì‹œê°„)"
                elif lag_period == 24:
                    lag_desc = "24ì‹œê°„ ì „ (ì „ì¼ ë™ì‹œê°„)"
                elif lag_period == 168:
                    lag_desc = "168ì‹œê°„ ì „ (ì „ì£¼ ë™ì‹œê°„)"
                else:
                    lag_desc = f"{lag_period}ì‹œê°„ ì „"
                
                print(f"  - ìµœê³  ì¤‘ìš”ë„ Lag: {lag_desc} (ì¤‘ìš”ë„: {top_lag['importance']:.4f})")
                
                # ë‹¨ê¸° vs ì¥ê¸° lag ë¹„êµ
                short_term_lags = lag_features[lag_features['feature'].str.contains(r'lag_[1-6]$', regex=True)]
                long_term_lags = lag_features[lag_features['feature'].str.contains(r'lag_(24|168)$', regex=True)]
                
                if len(short_term_lags) > 0 and len(long_term_lags) > 0:
                    short_importance = short_term_lags['importance'].sum()
                    long_importance = long_term_lags['importance'].sum()
                    
                    print(f"  - ë‹¨ê¸° Lag (1-6ì‹œê°„): {short_importance:.4f}")
                    print(f"  - ì¥ê¸° Lag (24, 168ì‹œê°„): {long_importance:.4f}")
                    
                    if short_importance > long_importance:
                        temporal_pattern = "ë‹¨ê¸° íŒ¨í„´ ì¤‘ì‹¬"
                    else:
                        temporal_pattern = "ì¥ê¸° íŒ¨í„´ ì¤‘ì‹¬"
                    print(f"  - ì£¼ìš” ì‹œê°„ì  íŒ¨í„´: {temporal_pattern}")
        
        print(f"\nğŸ’¡ ì‹œê³„ì—´ ëª¨ë¸ë§ ê¶Œì¥ì‚¬í•­:")
        if best_result['r2'] > 0.7:
            print(f"  âœ… ëª¨ë¸ì´ ì‹œê°„ì  íŒ¨í„´ì„ ì˜ í•™ìŠµí–ˆìŠµë‹ˆë‹¤")
            print(f"  âœ… 2022ë…„ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜")
            
            if hasattr(best_model, 'feature_importances_'):
                if total_temporal_importance > 0.3:
                    print(f"  ğŸ•’ Lag featuresê°€ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤")
                    print(f"  ğŸ“ˆ ì¶”ê°€ ê°œì„ : ë” ë§ì€ lag periods, ê³„ì ˆì„± lag ì¶”ê°€")
                else:
                    print(f"  âš ï¸ Lag featuresì˜ í™œìš©ë„ê°€ ë‚®ìŠµë‹ˆë‹¤")
                    print(f"  ğŸ”§ ê°œì„  ë°©í–¥: lag period ì¡°ì •, ë” ê¸´ ì‹œê³„ì—´ ë°ì´í„° í™œìš©")
            
            print(f"  ğŸš€ ì¶”ê°€ ìµœì í™”: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ì•™ìƒë¸”")
        else:
            print(f"  âš ï¸ ì‹œê°„ì  ì¼ë°˜í™” ì„±ëŠ¥ ê°œì„  í•„ìš”")
            print(f"  ğŸ“ˆ ì¶”ì²œ ë°©ë²•:")
            print(f"    - ë” ë‹¤ì–‘í•œ lag periods ì‹¤í—˜")
            print(f"    - ê³„ì ˆì„± decomposition í™œìš©")
            print(f"    - ì™¸ë¶€ ë°ì´í„° (ê³µíœ´ì¼, ì´ë²¤íŠ¸) ì¶”ê°€")
            print(f"    - LSTM/GRU ë“± ìˆœí™˜ ì‹ ê²½ë§ ëª¨ë¸ ì‹œë„")
            
            if hasattr(best_model, 'feature_importances_') and len(lag_features) > 0:
                print(f"    - Lag features ìµœì í™” ({len(lag_features)}ê°œ í˜„ì¬ ì‚¬ìš© ì¤‘)")
        
        print(f"\nğŸ”® ëª¨ë¸ í™œìš© ë°©í–¥:")
        print(f"  - ì‹¤ì‹œê°„ í˜¼ì¡ë„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•")
        print(f"  - ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„ íŒ¨í„´ ë¶„ì„")
        print(f"  - êµí†µ ì •ì±… ìˆ˜ë¦½ ì§€ì› ë„êµ¬")
        if best_result['r2'] > 0.7:
            print(f"  - ìš´ì˜ ì‹œìŠ¤í…œ ë„ì… ì¤€ë¹„ ì™„ë£Œ")
        else:
            print(f"  - ì¶”ê°€ ê°œì„  í›„ ìš´ì˜ ì‹œìŠ¤í…œ ë„ì… ê¶Œì¥")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì‹œê³„ì—´ ê²€ì¦ ì‹œì‘: 2021ë…„ í›ˆë ¨ â†’ 2022ë…„ ê²€ì¦")
    print("=" * 60)
    
    # ê²€ì¦ê¸° ì´ˆê¸°í™”
    validator = TimeSeriesValidator()
    
    # 1. ë°ì´í„° ë¡œë“œ
    if not validator.load_data(train_years=['21'], test_year='23', sample_size=5000000):
        return None
    
    # 2. íŠ¹ì„± ì¤€ë¹„
    X_train, y_train, X_test, y_test = validator.prepare_features()
    
    # 3. ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦
    results = validator.train_and_validate(X_train, y_train, X_test, y_test)
    
    # 4. ì‹œê°„ì  ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„
    validator.analyze_temporal_generalization()
    
    # 5. ê²°ê³¼ ì‹œê°í™”
    validator.plot_validation_results()
    
    # 6. ì¸ì‚¬ì´íŠ¸ ìƒì„±
    validator.generate_insights()
    
    print(f"\nğŸ‰ ì‹œê³„ì—´ ê²€ì¦ ì™„ë£Œ!")
    print(f"ê²°ê³¼ ì´ë¯¸ì§€: time_series_validation_with_lag.png")
    
    return validator

if __name__ == "__main__":
    validator = main() 
